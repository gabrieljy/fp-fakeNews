{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0c2fd3-075f-4b1e-b47f-c2328779e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE IMPORT STATEMENT FOR TORCH (if u want cuda)\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78248501-0f85-4758-bbf3-aa7e72f1454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded & pre-processed.\n",
      "Train shape: (10240, 15)\n",
      "Valid shape: (1284, 15)\n",
      "Test  shape: (1267, 15)\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# main_pipeline.ipynb\n",
    "# \n",
    "# A single notebook to demonstrate the entire\n",
    "# pipeline, from preprocessing to training,\n",
    "# evaluation, and a sample prediction.\n",
    "##############################################\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "##############################################\n",
    "# 1) DATA PREPROCESSING\n",
    "##############################################\n",
    "\n",
    "# For demonstration, we might just replicate or call \n",
    "# the critical steps from your preprocess_data.ipynb\n",
    "\n",
    "# If your 'preprocess_data.ipynb' is short, you could copy \n",
    "# the relevant code here. For instance:\n",
    "data_dir = os.path.join(os.getcwd(), \"data/liar_dataset\")  # Adjust if needed\n",
    "train_path = os.path.join(data_dir, \"train.tsv\")\n",
    "valid_path = os.path.join(data_dir, \"valid.tsv\")\n",
    "test_path  = os.path.join(data_dir, \"test.tsv\")\n",
    "\n",
    "def map_label_to_binary(label_str):\n",
    "    # example of your mapping function\n",
    "    fake_labels = [\"pants-fire\", \"false\", \"barely-true\"]\n",
    "    if label_str in fake_labels:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Load & preprocess\n",
    "train_df = pd.read_csv(train_path, sep='\\t', header=None, names=[\n",
    "    \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_jobtitle\",\n",
    "    \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\",\n",
    "    \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"\n",
    "])\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', header=None, names=train_df.columns)\n",
    "test_df  = pd.read_csv(test_path,  sep='\\t', header=None, names=train_df.columns)\n",
    "\n",
    "train_df['binary_label'] = train_df['label'].apply(map_label_to_binary)\n",
    "valid_df['binary_label'] = valid_df['label'].apply(map_label_to_binary)\n",
    "test_df['binary_label']  = test_df['label'].apply(map_label_to_binary)\n",
    "\n",
    "print(\"Data loaded & pre-processed.\")\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Valid shape: {valid_df.shape}\")\n",
    "print(f\"Test  shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fe4f168-a38a-46d2-96dc-ff9ad893ab14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Gabriel\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [640/640 03:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>0.663376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileBERT model and tokenizer saved to 'mobilebert_output'.\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# 2) MODEL TRAINING: FINE-TUNE MOBILEBERT\n",
    "##############################################\n",
    "\n",
    "# We'll replicate a simplified training approach here.\n",
    "# If 'train_mobileBERT.ipynb' has many lines, you can adapt them below:\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model_name = \"google/mobilebert-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class LIARDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.texts  = df['statement'].tolist()\n",
    "        self.labels = df['binary_label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = LIARDataset(train_df, tokenizer)\n",
    "valid_dataset = LIARDataset(valid_df, tokenizer)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"mobilebert_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,  # use 1 or 2 for a quick demo\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "tokenizer.save_pretrained(\"mobilebert_output\")\n",
    "print(\"MobileBERT model and tokenizer saved to 'mobilebert_output'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4ca874-4213-45f7-b9a1-6b7d51c2b229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation Metrics:\n",
      "Accuracy:  0.627\n",
      "Precision: 0.624\n",
      "Recall:    0.847\n",
      "F1 Score:  0.719\n",
      "Confusion Matrix:\n",
      "[[189 364]\n",
      " [109 605]]\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# 3) EVALUATION\n",
    "##############################################\n",
    "\n",
    "# We'll do a quick pass on the test set to see metrics.\n",
    "# You can replicate your more detailed code from evaluation.ipynb as needed.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "test_dataset = LIARDataset(test_df, tokenizer)\n",
    "\n",
    "# We'll have the trainer do predictions for convenience\n",
    "preds_output = trainer.predict(test_dataset)\n",
    "pred_labels = preds_output.predictions.argmax(axis=1)\n",
    "true_labels = preds_output.label_ids\n",
    "\n",
    "acc = accuracy_score(true_labels, pred_labels)\n",
    "prec = precision_score(true_labels, pred_labels, average='binary')\n",
    "rec = recall_score(true_labels, pred_labels, average='binary')\n",
    "f1 = f1_score(true_labels, pred_labels, average='binary')\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "print(\"\\nTest Set Evaluation Metrics:\")\n",
    "print(f\"Accuracy:  {acc:.3f}\")\n",
    "print(f\"Precision: {prec:.3f}\")\n",
    "print(f\"Recall:    {rec:.3f}\")\n",
    "print(f\"F1 Score:  {f1:.3f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff2978a-24b4-4eb1-9de2-8ad6f28761c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Predictions:\n",
      "Statement: Obama was born in Kenya.\n",
      "Prediction: FAKE (confidence: 0.59)\n",
      "\n",
      "Statement: The sky is blue.\n",
      "Prediction: FAKE (confidence: 0.55)\n",
      "\n",
      "Statement: Elvis Presley is still alive.\n",
      "Prediction: FAKE (confidence: 0.58)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# 4) QUICK DEMO PREDICTION (CLI-Like)\n",
    "##############################################\n",
    "\n",
    "# Let's emulate your 'cli_predict.ipynb' with a sample statement\n",
    "def predict_fake_news(statement, model_dir='mobilebert_output'):\n",
    "    loaded_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    loaded_model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "    loaded_model.eval()\n",
    "    \n",
    "    inputs = loaded_tokenizer(statement, return_tensors='pt',\n",
    "                              max_length=128, padding='max_length', truncation=True)\n",
    "    with torch.no_grad():\n",
    "        out = loaded_model(**inputs)\n",
    "    logits = out.logits\n",
    "    probs = torch.softmax(logits, dim=1).squeeze()\n",
    "    label_idx = torch.argmax(probs).item()\n",
    "    \n",
    "    label_str = \"REAL\" if label_idx == 1 else \"FAKE\"\n",
    "    confidence = probs[label_idx].item()\n",
    "    return label_str, confidence\n",
    "\n",
    "sample_statements = [\n",
    "    \"Obama was born in Kenya.\",\n",
    "    \"The sky is blue.\",\n",
    "    \"Elvis Presley is still alive.\"\n",
    "]\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "for stmt in sample_statements:\n",
    "    pred_label, conf = predict_fake_news(stmt)\n",
    "    print(f\"Statement: {stmt}\")\n",
    "    print(f\"Prediction: {pred_label} (confidence: {conf:.2f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dc8131-9e12-484a-b313-3e913b845e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
