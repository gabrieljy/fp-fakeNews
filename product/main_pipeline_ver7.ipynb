{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f3e53b-5ebe-4121-afc6-45b2a5200b00",
   "metadata": {},
   "source": [
    "# Fake News Detection using Multi-Task Learning & BERT\n",
    "\n",
    "**Notebook Structure:**\n",
    "\n",
    "1.  **Library Imports:** Loading necessary libraries.\n",
    "2.  **Centrally Defined Configuration & Parameters:** Setting up paths, hyperparameters, seeds, and device.\n",
    "3.  **Utility Functions & Classes:** Definitions for:\n",
    "    *   Data Loading & Preprocessing\n",
    "    *   Exploratory Data Analysis (EDA)\n",
    "    *   Functions & Classes for BERT & Multi-Task Learning Model\n",
    "    *   Training\n",
    "    *   Evaluation\n",
    "4.  **Main Workflow:** Step-by-step execution:\n",
    "    *   Loading Data\n",
    "    *   Preprocessing & EDA\n",
    "    *   Domain Classifier Training (ISOT vs. WELFake)\n",
    "    *   Data Splitting\n",
    "    *   Baseline Model (TF-IDF + Logistic Regression) \n",
    "    *   BERT & Multi-Task Learning (MTL) Setup\n",
    "    *   MTL Model Training\n",
    "    *   MTL Model Evaluation\n",
    "    *   Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c45e6-bed3-4ba6-b8ae-d3a98bc95093",
   "metadata": {},
   "source": [
    "# Library Imports\n",
    "\n",
    "The following code block contains all import statements to import all necessary Python libraries for the project. \n",
    "\n",
    "I group them by functionality (core data handling, machine learning, deep learning (PyTorch/Transformers), visualisation, and standard utility functions). Additionally, I also handle warning suppression and set a plot style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9398eb23-c357-4899-b850-04756422ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Library Imports\n",
    "\n",
    "# core libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import warnings\n",
    "import functools\n",
    "import traceback\n",
    "\n",
    "# data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# machine learning-related\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score, roc_auc_score, roc_curve)\n",
    "\n",
    "# deep learning-related\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (DistilBertForSequenceClassification, # Keep for potential single model use later?\n",
    "                          DistilBertModel, # Used in MTL\n",
    "                          DistilBertTokenizer, AdamW,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud \n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# configuration \n",
    "# warnings.filterwarnings(\"ignore\") # Suppress simple warnings\n",
    "sns.set_style('darkgrid') # Set default plot style\n",
    "\n",
    "# for GUI\n",
    "import gradio as gr\n",
    "import lime\n",
    "import lime.lime_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f59f0-d313-47c9-a13e-31ce332c1ccb",
   "metadata": {},
   "source": [
    "# Centrally Defined Configuration & Parameters\n",
    "\n",
    "The following code block centralises all configuration settings and hyperparameters for the project. \n",
    "\n",
    "This includes dataset paths, model parameters (e.g. BERT model name, sequence length, batch size, learning rate, epochs), the random seed for reproducibility, and device selection (GPU or CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e904a-cd15-471d-a15a-34763aaf1743",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Centrally Defined Configuration & Parameters\n",
    "\n",
    "# dataset paths\n",
    "ISOT_DATA_PATH = 'data/isot_dataset'\n",
    "WELFAKE_DATA_PATH = 'data/welfake_dataset'\n",
    "ISOT_TRUE_FILE = os.path.join(ISOT_DATA_PATH, 'True.csv')\n",
    "ISOT_FAKE_FILE = os.path.join(ISOT_DATA_PATH, 'Fake.csv')\n",
    "WELFAKE_FILE = os.path.join(WELFAKE_DATA_PATH, 'WELFake_Dataset.csv') \n",
    "\n",
    "# model configuration\n",
    "BERT_MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LEN = 256   \n",
    "BATCH_SIZE = 16   \n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS_MTL = 3 \n",
    "\n",
    "# training random seed \n",
    "SEED = 42 \n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# device setup \n",
    "# use gpu if available, otherwise cpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# output settings\n",
    "# define paths for saving models or results if needed later\n",
    "MODEL_CHECKPOINT_DIR = './model_checkpoints_mtl'\n",
    "RESULTS_DIR = './results'\n",
    "# create directories if they don't exist\n",
    "os.makedirs(MODEL_CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "# defining path for the best MTL model checkpoint\n",
    "BEST_MTL_MODEL_PATH = os.path.join(MODEL_CHECKPOINT_DIR, 'mtl_model_best.bin')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28931a4-128f-4ee5-8085-02521c980695",
   "metadata": {},
   "source": [
    "# Utility Functions & Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6d31b-aacd-47d9-acbc-b8e543178190",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing\n",
    "\n",
    "The following code block contains functions dedicated to handling the datasets:\n",
    "\n",
    "1.  **load_isot_dataset**: This function reads `True.csv` and `Fake.csv` from the ISOT dataset path, adds a 'label' column (1 for Real, 0 for Fake), adds a 'source' column, and combines them into a single Pandas DataFrame.\n",
    "\n",
    "2.  **load_welfake_dataset**: This function reads `WELFake_Dataset.csv` from the WELFake dataset path, attempts to identify and standardize the 'label' column (ensuring 1 for Real, 0 for Fake), removes unnamed index columns, and adds a 'source' column. Includes error handling for missing files or columns.\n",
    "\n",
    "3.  **combine_text**: This helper function concatenates 'title' and 'text' columns into a 'full_text' column and handles potential missing values (NaN).\n",
    "\n",
    "4.  **clean_text**: This function performs basic text cleaning operations: converts text to lowercase, removes URLs, HTML tags, text in square brackets, punctuation (except apostrophes within words), numbers, and extra whitespace.\n",
    "\n",
    "5.  **split_data**: This function splits a DataFrame into training, validation, and test sets using stratified sampling based on the 'label' column to maintain class distribution. Includes handling for cases where stratification might not be possible due to small class sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3560280-def0-4bf5-b659-5980a18d6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading & Preprocessing\n",
    "\n",
    "# function to load isot dataset, combine true/fake news, and add labels/source\n",
    "# arguments:\n",
    "#   true_file_path - path to the CSV containing true news\n",
    "#   fake_file_path - path to the CSV containing fake news\n",
    "# returns:\n",
    "#   pandas dataframe with combined isot data, or empty dataframe on error\n",
    "def load_isot_dataset(true_file_path, fake_file_path):\n",
    "    try:\n",
    "        # step 1: read csv files\n",
    "        df_true = pd.read_csv(true_file_path)\n",
    "        df_fake = pd.read_csv(fake_file_path)\n",
    "\n",
    "        # step 2: add label column (1 for real, 0 for fake)\n",
    "        df_true['label'] = 1\n",
    "        df_fake['label'] = 0\n",
    "\n",
    "        # step 3: combine dataframes\n",
    "        df_isot = pd.concat([df_true, df_fake], ignore_index=True)\n",
    "\n",
    "        # step 4: add source column\n",
    "        df_isot['source'] = 'ISOT'\n",
    "\n",
    "        print(f\"isot dataset: loaded {len(df_true)} true and {len(df_fake)} fake articles.\")\n",
    "        return df_isot\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"error loading isot dataset: {e}. please check file paths.\")\n",
    "        return pd.DataFrame() # return empty dataframe on error\n",
    "    except Exception as e: # catch other potential read errors\n",
    "        print(f\"an unexpected error occurred loading isot: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "# function to load welfake dataset, standardise the label, and add source\n",
    "# arguments:\n",
    "#   file_path - path to the welfake CSV file\n",
    "# returns:\n",
    "#   pandas dataframe with welfake data, or empty dataframe on error\n",
    "def load_welfake_dataset(file_path):\n",
    "    try:\n",
    "        # step 1: read csv file\n",
    "        df_welfake = pd.read_csv(file_path)\n",
    "\n",
    "        # step 2: remove potential unnamed index column\n",
    "        if 'Unnamed: 0' in df_welfake.columns:\n",
    "            df_welfake = df_welfake.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "        # step 3: standardise label column name (try 'Label' if 'label' not found, which idk why sometimes happens)\n",
    "        if 'label' not in df_welfake.columns and 'Label' in df_welfake.columns:\n",
    "             df_welfake = df_welfake.rename(columns={'Label': 'label'})\n",
    "\n",
    "        # step 4: check if label column exists\n",
    "        if 'label' not in df_welfake.columns:\n",
    "            print(\"error: 'label' column not found in welfake dataset. cannot proceed.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # step 5: ensure labels are integers 0 or 1 (welfake uses 1 for real, 0 for fake)\n",
    "        try:\n",
    "            df_welfake['label'] = df_welfake['label'].astype(int)\n",
    "            # verify if labels are indeed 0 and 1\n",
    "            if not df_welfake['label'].isin([0, 1]).all():\n",
    "                 print(\"warning: 'label' column in welfake contains values other than 0 and 1. check data.\")\n",
    "        except ValueError:\n",
    "             print(\"error: could not convert 'label' column in welfake to integer. check data.\")\n",
    "             return pd.DataFrame()\n",
    "\n",
    "        # step 6: add source column\n",
    "        df_welfake['source'] = 'WELFake'\n",
    "\n",
    "        print(f\"welfake dataset: loaded {len(df_welfake)} articles.\")\n",
    "        return df_welfake\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"error loading welfake dataset: {e}. please check file path.\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e: # catch other potential errors during loading/processing\n",
    "        print(f\"an unexpected error occurred loading welfake: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "# function to combine title and text safely and handle potential NaN values\n",
    "# arguments:\n",
    "#   title - content of the title column\n",
    "#   text - content of the text column\n",
    "# returns:\n",
    "#   combined string\n",
    "def combine_text(title, text):\n",
    "    # step 1: convert inputs to string, handling nan\n",
    "    title_str = str(title) if pd.notna(title) else ''\n",
    "    text_str = str(text) if pd.notna(text) else ''\n",
    "    # step 2: add a space separator only if both parts exist\n",
    "    separator = ' ' if title_str and text_str else ''\n",
    "    # step 3: return concatenated string\n",
    "    return title_str + separator + text_str\n",
    "\n",
    "\n",
    "\n",
    "# function to perform basic text cleaning: lowercase, remove URLs, HTML, brackets,\n",
    "# punctuation (keep internal apostrophes), numbers, extra whitespace.\n",
    "# arguments:\n",
    "#   text - input string\n",
    "# returns:\n",
    "#   cleaned string\n",
    "def clean_text(text):\n",
    "    # step 1: handle non-string input\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # step 2: convert to lowercase\n",
    "    text = text.lower()\n",
    "    # step 3: remove text in square brackets\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    # step 4: remove urls\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    # step 5: remove html tags\n",
    "    text = re.sub(r'<.*?>+', '', text)\n",
    "    # step 6: remove punctuation but keep apostrophes within words\n",
    "    # temporarily replace internal apostrophes to preserve them\n",
    "    text = re.sub(r\"(\\w)'(\\w)\", r\"\\1@@APOS@@\\2\", text)\n",
    "    # remove all punctuation characters\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    # restore internal apostrophes\n",
    "    text = text.replace(\"@@APOS@@\", \"'\")\n",
    "    # step 7: remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # step 8: remove extra whitespace and strip leading/trailing spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# function to split dataframe into train, validation, and test sets with stratification\n",
    "# arguments:\n",
    "#   df - dataframe to split\n",
    "#   test_size - proportion for test set (default 0.2)\n",
    "#   val_size_of_train - proportion of training set to use for validation (default 0.1)\n",
    "#   stratify_col - column name to stratify on (default 'label')\n",
    "#   random_state - seed for reproducibility\n",
    "# returns:\n",
    "#   train_df, val_df, test_df as pandas dataframes\n",
    "def split_data(df, test_size=0.2, val_size_of_train=0.1, stratify_col='label', random_state=SEED):\n",
    "    # step 1: validate input dataframe and stratify column\n",
    "    if df.empty or stratify_col not in df.columns:\n",
    "        print(f\"cannot split empty dataframe or missing stratify column '{stratify_col}'.\")\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # step 2: check if stratification is possible for the first split\n",
    "    counts = df[stratify_col].value_counts()\n",
    "    if (counts < 2).any():\n",
    "        print(f\"warning: some classes in '{stratify_col}' have < 2 samples. stratification might fail. trying without.\")\n",
    "        stratify_values = None\n",
    "    else:\n",
    "        stratify_values = df[stratify_col]\n",
    "\n",
    "    # step 3: split into train+validation and test\n",
    "    try:\n",
    "        train_val_df, test_df = train_test_split(\n",
    "            df,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            stratify=stratify_values\n",
    "        )\n",
    "    except ValueError as e:\n",
    "         print(f\"stratified split failed: {e}. splitting without stratification.\")\n",
    "         train_val_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # step 4: calculate adjusted validation size for the second split\n",
    "    val_size_adjusted = val_size_of_train / (1 - test_size)\n",
    "\n",
    "    # step 5: check if stratification is possible for the second split\n",
    "    stratify_train_val = None\n",
    "    if stratify_values is not None: # only try if first stratification worked\n",
    "        train_val_counts = train_val_df[stratify_col].value_counts()\n",
    "        if not (train_val_counts < 2).any():\n",
    "             stratify_train_val = train_val_df[stratify_col]\n",
    "        else:\n",
    "             print(f\"warning: stratification for train/val split might fail (classes < 2).\")\n",
    "\n",
    "    # step 6: split train+validation into train and validation\n",
    "    try:\n",
    "        train_df, val_df = train_test_split(\n",
    "            train_val_df,\n",
    "            test_size=val_size_adjusted,\n",
    "            random_state=random_state,\n",
    "            stratify=stratify_train_val\n",
    "        )\n",
    "    except ValueError as e:\n",
    "         print(f\"stratified train/val split failed: {e}. splitting without stratification.\")\n",
    "         train_df, val_df = train_test_split(train_val_df, test_size=val_size_adjusted, random_state=random_state)\n",
    "\n",
    "    print(f\"dataset split sizes: train={len(train_df)}, validation={len(val_df)}, test={len(test_df)}\")\n",
    "    return train_df, val_df, test_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3c02f-abbc-49ac-8a61-c9af83db95ec",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "The following code block contains functions specifically for Exploratory Data Analysis:\n",
    "\n",
    "1.  **generate_word_cloud**: This function creates and displays a word cloud visualisation from the text content of a given DataFrame, filtered by a specific label (e.g., Fake or Real news) and dataset source. It helps visualise the most frequent words in different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45166dd9-2f34-42f1-b965-af5be13991a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate a word cloud visualisation for text based on label\n",
    "# arguments:\n",
    "#   df - dataframe containing 'cleaned_text' and 'label'\n",
    "#   label_val - the label value to filter by (0 or 1)\n",
    "#   label_name - string name for the label (e.g., 'Fake', 'Real')\n",
    "#   source_name - string name for the data source (e.g., 'ISOT', 'WELFake')\n",
    "# returns:\n",
    "#   None (displays plot)\n",
    "def generate_word_cloud(df, label_val, label_name, source_name):\n",
    "    # step 1: filter data for the specified label and source\n",
    "    text_series = df.loc[df['label'] == label_val, 'cleaned_text']\n",
    "    if text_series.empty:\n",
    "        print(f\"no text found for {label_name} news in {source_name}.\")\n",
    "        return\n",
    "    text = ' '.join(text_series)\n",
    "    if not text.strip(): # check if the joined text is empty or just whitespace\n",
    "         print(f\"joined text is empty for {label_name} news in {source_name} after filtering.\")\n",
    "         return\n",
    "\n",
    "    # step 2: generate wordcloud object\n",
    "    try:\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
    "                              # consider adding stopwords='english' if needed, or max_words=100\n",
    "                             ).generate(text)\n",
    "    except ValueError as e:\n",
    "         print(f\"could not generate word cloud for {label_name} in {source_name} (likely empty text after filtering stopwords): {e}\")\n",
    "         return\n",
    "\n",
    "    # step 3: display the plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud for {label_name} News ({source_name})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1e9ef-ea2c-45ac-837a-f896bf8c5f81",
   "metadata": {},
   "source": [
    "## Functions & Classes for BERT & Multi-Task Learning Model\n",
    "\n",
    "The following code block defines components related to the BERT and Multi-Task Learning (MTL) model:\n",
    "\n",
    "1.  **NewsDataset (Class)**: This custom PyTorch `Dataset` class takes texts and labels, tokenises the text using the provided BERT tokeniser (padding/truncating to `MAX_LEN`), and returns formatted dictionaries containing `input_ids`, `attention_mask`, and `labels` as PyTorch tensors, ready for input into a BERT model.\n",
    "\n",
    "2.  **create_dataset**: This helper function instantiates the `NewsDataset` from a DataFrame.\n",
    "\n",
    "3.  **create_data_loader**: This helper function creates PyTorch `DataLoader` instances from `NewsDataset` objects, which handle batching and shuffling of data during training and evaluation.\n",
    "\n",
    "4.  **DistilBertMTL (Class)**: This class defines the Multi-Task Learning model architecture. It uses a shared `DistilBertModel` base and adds two separate linear classification heads (`head_A` and `head_B`), one for each task (ISOT and WELFake). The `forward` method directs the input through the shared base and then selects the appropriate head based on a `task_id` argument. Includes dropout for regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854a284-e631-4b89-8dce-fe816e62d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions & Classes for BERT & Multi-Task Learning Model\n",
    "\n",
    "# this class formats the data correctly for BERT input. it takes texts and labels,\n",
    "# tokenises the text using the provided BERT tokeniser (padding/truncating to max_len),\n",
    "# and returns formatted dictionaries containing input_ids, attention_mask, and labels\n",
    "# as PyTorch tensors.\n",
    "class NewsDataset(Dataset):\n",
    "    # constructor\n",
    "    # arguments:\n",
    "    #   texts - list or series of text strings\n",
    "    #   labels - list or series of labels (0 or 1)\n",
    "    #   tokenizer - the bert tokenizer instance\n",
    "    #   max_len - maximum sequence length for padding/truncation\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    # function to get item count\n",
    "    # returns: integer length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    # function to get an item (tokenised text and label) by index\n",
    "    # arguments:\n",
    "    #   item - index of the item to retrieve\n",
    "    # returns:\n",
    "    #   dictionary with tokenised 'input_ids', 'attention_mask', and 'labels' tensor\n",
    "    def __getitem__(self, item):\n",
    "        # step 1: get text and label for the item\n",
    "        text = str(self.texts[item]) # ensure text is string\n",
    "        label = self.labels[item]\n",
    "\n",
    "        # step 2: tokenise text using the provided tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,    # add '[CLS]' and '[SEP]'\n",
    "            max_length=self.max_len,    # pad & truncate\n",
    "            padding='max_length',       # pad to max_length\n",
    "            truncation=True,            # truncate to max_length\n",
    "            return_attention_mask=True, # create attention masks\n",
    "            return_tensors='pt',        # return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        # step 3: format output dictionary\n",
    "        return {\n",
    "            # 'text': text, # optionally keep original text for debugging\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# helper function to create a NewsDataset object from a dataframe\n",
    "# arguments:\n",
    "#   df - dataframe containing 'cleaned_text' and 'label' columns\n",
    "#   tokenizer - the bert tokenizer instance\n",
    "#   max_len - maximum sequence length\n",
    "# returns:\n",
    "#   NewsDataset object or None if df is empty/invalid\n",
    "def create_dataset(df, tokenizer, max_len):\n",
    "    # step 1: validate input dataframe\n",
    "    if df is None or df.empty:\n",
    "        print(\"warning: dataframe is none or empty, cannot create dataset.\")\n",
    "        return None\n",
    "    if 'cleaned_text' not in df.columns or 'label' not in df.columns:\n",
    "         print(\"error: missing 'cleaned_text' or 'label' column in dataframe.\")\n",
    "         return None\n",
    "\n",
    "    # step 2: reset index for safe access by integer index in __getitem__\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # step 3: instantiate and return the dataset object\n",
    "    return NewsDataset(\n",
    "        texts=df['cleaned_text'].tolist(),\n",
    "        labels=df['label'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# helper function to create a DataLoader instance from a NewsDataset object\n",
    "# arguments:\n",
    "#   ds - NewsDataset object\n",
    "#   batch_size - batch size for the dataloader\n",
    "#   num_workers - number of worker processes for loading data (default 0)\n",
    "#   shuffle - whether to shuffle data (True for train, False for val/test)\n",
    "# returns:\n",
    "#   DataLoader object or None if dataset is None\n",
    "def create_data_loader(ds, batch_size, num_workers=0, shuffle=False):\n",
    "    # step 1: validate input dataset\n",
    "    if ds is None:\n",
    "        print(\"warning: dataset is none, cannot create dataloader.\")\n",
    "        return None\n",
    "    # step 2: instantiate and return the dataloader object\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers, # set higher (e.g., 2 or 4) if cpu allows for faster loading\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# the DistilBertMTL class defines the DistilBERT MTL architecture. It uses a shared DistilBERT base\n",
    "# and adds two separate linear classification heads (head_A, head_B) for each task.\n",
    "# this also includes dropout for regularisation.\n",
    "class DistilBertMTL(nn.Module):\n",
    "    # constructor\n",
    "    # arguments:\n",
    "    #   bert_model_name - name of the pretrained distilbert model (default from config)\n",
    "    def __init__(self, bert_model_name=BERT_MODEL_NAME):\n",
    "        super(DistilBertMTL, self).__init__()\n",
    "        # step 1: load shared bert base model (without the classification head)\n",
    "        self.bert = DistilBertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "        # step 2: define task-specific classification heads\n",
    "        hidden_size = self.bert.config.hidden_size # get hidden dimension from bert config\n",
    "\n",
    "        # head for task a (e.g., isot) - maps hidden size to 2 output classes (fake/real)\n",
    "        self.head_A = nn.Linear(hidden_size, 2)\n",
    "\n",
    "        # head for task b (e.g., welfake) - maps hidden size to 2 output classes\n",
    "        self.head_B = nn.Linear(hidden_size, 2)\n",
    "\n",
    "        # step 3: define dropout layer for regularisation during classification\n",
    "        # use dropout rate from distilbert config for consistency\n",
    "        self.dropout = nn.Dropout(self.bert.config.seq_classif_dropout)\n",
    "\n",
    "    # forward pass function for the mtl model. directs input through the shared base\n",
    "    # and selects the appropriate head based on task_id.\n",
    "    # arguments:\n",
    "    #   input_ids - token ids tensor from tokenizer\n",
    "    #   attention_mask - attention mask tensor from tokenizer\n",
    "    #   task_id - identifier for the task ('A' or 'B') specifying which head to use\n",
    "    # returns:\n",
    "    #   logits tensor from the specified task head\n",
    "    def forward(self, input_ids, attention_mask, task_id):\n",
    "        # step 1: pass input through the shared bert base\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # step 2: get the pooled output (hidden state corresponding to the [CLS] token)\n",
    "        # shape: (batch_size, hidden_size)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # step 3: apply dropout to the pooled output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        # step 4: pass the pooled output through the appropriate task-specific head\n",
    "        if task_id == 'A':\n",
    "            logits = self.head_A(pooled_output)\n",
    "        elif task_id == 'B':\n",
    "            logits = self.head_B(pooled_output)\n",
    "        else:\n",
    "            # raise error for invalid task id\n",
    "            raise ValueError(\"invalid task_id provided. must be 'A' or 'B'.\")\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83738c3c-6449-43fa-b44a-1dc341fb18f4",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "The following code block defines the core function for training the models:\n",
    "\n",
    "1.  **train_epoch_mtl**: This function performs one epoch of training for the `DistilBertMTL` model. It interleaves batches from the two task-specific DataLoaders (Task A and Task B). For each batch, it performs a forward pass using the correct `task_id`, calculates the loss, performs a backward pass to update weights (affecting both the specific head and the shared BERT base), and updates the optimizer and learning rate scheduler. It tracks and returns the average loss and accuracy across all batches in the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ea3fd-9df6-44c4-b555-3a0cb88d658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "\n",
    "# function for one mtl training epoch, interleaving batches from two loaders.\n",
    "# performs forward/backward passes, updates weights, and tracks metrics.\n",
    "# arguments:\n",
    "#   model - the pytorch mtl model to train\n",
    "#   loader_A - dataloader for task a training data (e.g., isot)\n",
    "#   loader_B - dataloader for task b training data (e.g., welfake)\n",
    "#   loss_fn - the loss function (e.g., crossentropyloss)\n",
    "#   optimizer - the optimizer (e.g., adamw)\n",
    "#   device - the device to run on ('cuda' or 'cpu')\n",
    "#   scheduler - learning rate scheduler\n",
    "# returns:\n",
    "#   tuple: (average training loss for the epoch, average training accuracy for the epoch)\n",
    "def train_epoch_mtl(model, loader_A, loader_B, loss_fn, optimizer, device, scheduler, epoch_num, total_epochs):\n",
    "    # step 1: set model to training mode\n",
    "    model = model.train()\n",
    "\n",
    "    # step 2: setup iterators and batch tracking\n",
    "    iter_A = iter(loader_A)\n",
    "    iter_B = iter(loader_B)\n",
    "    len_A = len(loader_A)\n",
    "    len_B = len(loader_B)\n",
    "    total_batches = len_A + len_B # total number of batches to process in one epoch\n",
    "\n",
    "    # step 3: initialise metrics tracking for the epoch\n",
    "    total_loss = 0\n",
    "    total_correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # step 4: loop through combined batches from both loaders\n",
    "    pbar = tqdm(range(total_batches), desc=f'Epoch {epoch_num}/{total_epochs} Training', leave=True)\n",
    "    \n",
    "    for i in pbar:\n",
    "        # determine which loader to use for the current batch\n",
    "        # simple alternation strategy: try to alternate, take from A first if equal chance\n",
    "        \n",
    "        use_loader_A = (i % 2 == 0 and (i // 2) < len_A) or ((i // 2) >= len_B and (i - len_B) < len_A)\n",
    "\n",
    "        # get batch and corresponding task id\n",
    "        try:\n",
    "            if use_loader_A:\n",
    "                task_id = 'A'\n",
    "                batch = next(iter_A)\n",
    "            else:\n",
    "                task_id = 'B'\n",
    "                batch = next(iter_B)\n",
    "        except StopIteration:\n",
    "            # safeguard against potential StopIteration errors\n",
    "            print(f\"\\nwarning: stopiteration caught unexpectedly at batch {i+1}/{total_batches}.\")\n",
    "            continue\n",
    "        except Exception as e_batch: \n",
    "             break # Exit loop on error\n",
    "        \n",
    "        # move batch data to the designated device \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # perform one training step \n",
    "        optimizer.zero_grad() # zero gradients before forward pass\n",
    "\n",
    "        # forward pass through model, specifying the task id\n",
    "        logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            task_id=task_id\n",
    "        )\n",
    "\n",
    "        # calculate loss using the appropriate head's output\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        # backward pass to compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # apply gradient clipping to prevent exploding gradients\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # update model weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # update learning rate based on scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # track performance metrics for the batch\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_batch_preds = torch.sum(preds == labels).item()\n",
    "        total_loss += loss.item()\n",
    "        total_correct_predictions += correct_batch_preds\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # update tqdm postfix with current average loss/accuracy \n",
    "        # Calculate metrics *so far* within the epoch\n",
    "        current_avg_loss = total_loss / (i + 1)\n",
    "        current_avg_acc = total_correct_predictions / total_samples if total_samples > 0 else 0\n",
    "        pbar.set_postfix(loss=f'{current_avg_loss:.4f}', acc=f'{current_avg_acc:.4f}')\n",
    "\n",
    "\n",
    "    print() # add a newline after the epoch progress indicator finishes\n",
    "\n",
    "    # step 5: calculate average loss and accuracy for the entire epoch\n",
    "    avg_loss = total_loss / total_batches if total_batches > 0 else 0\n",
    "    avg_accuracy = total_correct_predictions / total_samples if total_samples > 0 else 0\n",
    "\n",
    "    return avg_loss, avg_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f8c2ea-8c38-4176-8c34-67c85ef5584b",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The following code block defines the core functions for evaluating the models:\n",
    "\n",
    "1.  **eval_model_mtl**: This function evaluates the `DistilBertMTL` model on a given DataLoader for a *specific task* (`task_id`). It sets the model to evaluation mode (`model.eval()`), disables gradient calculations (`torch.no_grad()`), iterates through the data, performs forward passes using the specified `task_id`, and calculates loss, accuracy, precision, recall, F1-score, and ROC AUC. It returns a dictionary containing these metrics along with predictions and probabilities.\n",
    "\n",
    "2.  **evaluate_predictions**: This function takes true labels, predicted labels, and predicted probabilities (for the positive class) as input, along with model/dataset names. It calculates and prints standard classification metrics (Accuracy, Precision, Recall, F1, ROC AUC) and displays a Classification Report, Confusion Matrix plot, and ROC Curve plot. It returns a dictionary of the calculated scalar metrics. This function is used for generating final reports for both baseline and MTL models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d3a42-1be4-4214-93d7-0a33294b2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the mtl model on validation/test data for a specific task.\n",
    "# disables gradients, performs forward pass using task_id, calculates metrics.\n",
    "# arguments:\n",
    "#   model - the pytorch mtl model to evaluate\n",
    "#   data_loader - dataloader for the evaluation data (validation or test)\n",
    "#   task_id - identifier for the task being evaluated ('A' or 'B')\n",
    "#   loss_fn - the loss function\n",
    "#   device - the device to run on\n",
    "# returns:\n",
    "#   dictionary containing evaluation metrics ('loss', 'accuracy', 'precision', 'recall', 'f1', 'roc_auc',\n",
    "#   'predictions', 'probabilities', 'true_labels') for the specified task, or None if evaluation fails.\n",
    "def eval_model_mtl(model, data_loader, task_id, loss_fn, device):\n",
    "    # step 1: validate dataloader input\n",
    "    if data_loader is None:\n",
    "        print(f\"skipping evaluation for task {task_id}: dataloader is none.\")\n",
    "        return None\n",
    "\n",
    "    # step 2: set model to evaluation mode (disables dropout, etc.)\n",
    "    model = model.eval()\n",
    "\n",
    "    # step 3: initialise lists to store results from all batches\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = [] # probabilities for the positive class (label 1)\n",
    "\n",
    "    # step 4: evaluation loop without gradient calculations\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            # move batch data to device\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass - MUST specify task_id for correct head\n",
    "            logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                task_id=task_id\n",
    "            )\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_fn(logits, labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # get predictions and probabilities\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            # calculate probability of the positive class (class 1)\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1]\n",
    "\n",
    "            # store batch results (move tensors to cpu before converting to numpy)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # step 5: check if any samples were processed\n",
    "    if not all_labels:\n",
    "        print(f\"no samples found for evaluation task {task_id}.\")\n",
    "        return None\n",
    "\n",
    "    # step 6: calculate overall metrics using stored results\n",
    "    avg_loss = np.mean(losses)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    try:\n",
    "         # calculate roc auc using probabilities of the positive class\n",
    "         roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "    except ValueError as e:\n",
    "         # handle cases where only one class is present in y_true (cannot calculate roc auc)\n",
    "         print(f\"could not calculate roc-auc for task {task_id} during eval: {e}. setting to 0.0\")\n",
    "         roc_auc = 0.0\n",
    "\n",
    "    # step 7: compile results into a dictionary\n",
    "    metrics = {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'predictions': all_preds, # include raw predictions if needed elsewhere\n",
    "        'probabilities': all_probs, # include probabilities for roc curve\n",
    "        'true_labels': all_labels # include true labels for consistency\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# function to evaluate prediction results, calculate metrics, print report, and generate plots\n",
    "# arguments:\n",
    "#   y_true - numpy array of true labels\n",
    "#   y_pred - numpy array of predicted labels\n",
    "#   y_proba - numpy array of predicted probabilities for the positive class (class 1)\n",
    "#   model_name - string name of the model (e.g., \"Baseline\", \"MTL Model\")\n",
    "#   dataset_name - string name of the dataset (e.g., \"ISOT Test\", \"WELFake Test\")\n",
    "# returns:\n",
    "#   dictionary of scalar metrics ('accuracy', 'precision', 'recall', 'f1', 'roc_auc')\n",
    "def evaluate_predictions(y_true, y_pred, y_proba, model_name, dataset_name):\n",
    "    # step 1: validate inputs to prevent errors with empty arrays\n",
    "    if y_true is None or y_pred is None or y_proba is None or len(y_true) == 0:\n",
    "        print(f\"skipping evaluation for {model_name} on {dataset_name}: missing or empty data arrays.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nevaluating {model_name} on {dataset_name}\")\n",
    "\n",
    "    # step 2: calculate standard classification metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    try:\n",
    "        # calculate roc auc using probabilities of the positive class\n",
    "        roc_auc = roc_auc_score(y_true, y_proba)\n",
    "    except ValueError as e:\n",
    "        # handle cases where roc auc cannot be calculated (e.g., only one class present)\n",
    "        print(f\"could not calculate roc-auc for {model_name} on {dataset_name}: {e}. setting to 0.0\")\n",
    "        roc_auc = 0.0\n",
    "\n",
    "    # step 3: print metrics and detailed classification report\n",
    "    print(f\"  accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  precision: {precision:.4f}\")\n",
    "    print(f\"  recall:    {recall:.4f}\")\n",
    "    print(f\"  f1-score:  {f1:.4f}\")\n",
    "    print(f\"  roc-auc:   {roc_auc:.4f}\")\n",
    "    print(\"\\n  classification report:\\n\", classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "    # step 4: generate and display confusion matrix plot\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake (0)', 'Real (1)'], yticklabels=['Fake (0)', 'Real (1)'])\n",
    "    plt.title(f'Confusion Matrix: {model_name} on {dataset_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    # optionally save the plot\n",
    "    # plt.savefig(os.path.join(RESULTS_DIR, f'cm_{model_name.replace(\" \", \"_\")}_{dataset_name.replace(\" \", \"_\")}.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # step 5: generate and display roc curve plot\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # plot diagonal line for reference\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve: {model_name} on {dataset_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    # optionally save the plot\n",
    "    # plt.savefig(os.path.join(RESULTS_DIR, f'roc_{model_name.replace(\" \", \"_\")}_{dataset_name.replace(\" \", \"_\")}.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # step 6: return calculated scalar metrics in a dictionary\n",
    "    return {\n",
    "        'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'roc_auc': roc_auc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39677e2b-73c3-4b5a-a3fa-49c6206a7dee",
   "metadata": {},
   "source": [
    "# Main Workflow\n",
    "\n",
    "The following section is dedicated to the execution of the main steps of the project by calling the functions defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe6ab8-ae90-4d88-a87c-a15595dd03e2",
   "metadata": {},
   "source": [
    "## 1. Load Datasets\n",
    "\n",
    "The following code block will load both ISOT and WELFake datasets, and display some basic information relating to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f40fe48-5fe3-4e19-a938-e7f171ce278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "df_isot = load_isot_dataset(ISOT_TRUE_FILE, ISOT_FAKE_FILE)\n",
    "df_welfake = load_welfake_dataset(WELFAKE_FILE)\n",
    "\n",
    "# combine title and text into 'full_text'\n",
    "if not df_isot.empty:\n",
    "    # check if required columns exist\n",
    "    if 'title' in df_isot.columns and 'text' in df_isot.columns:\n",
    "        df_isot['full_text'] = df_isot.apply(lambda row: combine_text(row['title'], row['text']), axis=1)\n",
    "        # keep only necessary columns + source\n",
    "        df_isot = df_isot[['full_text', 'label', 'source']].copy()\n",
    "        print(\"isot: combined 'title' and 'text' into 'full_text'.\")\n",
    "    else:\n",
    "        print(\"warning: 'title' or 'text' column missing in isot df. cannot create 'full_text'.\")\n",
    "        # fallback: use only 'text' if available\n",
    "        if 'text' in df_isot.columns:\n",
    "             df_isot['full_text'] = df_isot['text'].fillna('').astype(str)\n",
    "             df_isot = df_isot[['full_text', 'label', 'source']].copy()\n",
    "             print(\"isot: used only 'text' column as 'full_text'.\")\n",
    "        else:\n",
    "             print(\"error: neither 'title' nor 'text' found in isot df. cannot proceed with isot.\")\n",
    "             df_isot = pd.DataFrame() # make empty to prevent downstream errors\n",
    "\n",
    "\n",
    "if not df_welfake.empty:\n",
    "    # check if required columns exist\n",
    "    if 'title' in df_welfake.columns and 'text' in df_welfake.columns:\n",
    "        df_welfake['full_text'] = df_welfake.apply(lambda row: combine_text(row['title'], row['text']), axis=1)\n",
    "        df_welfake = df_welfake[['full_text', 'label', 'source']].copy()\n",
    "        print(\"welfake: combined 'title' and 'text' into 'full_text'.\")\n",
    "    else:\n",
    "         print(\"warning: 'title' or 'text' column missing in welfake df. cannot create 'full_text'.\")\n",
    "         # fallback: use only 'text' if available\n",
    "         if 'text' in df_welfake.columns:\n",
    "             df_welfake['full_text'] = df_welfake['text'].fillna('').astype(str)\n",
    "             df_welfake = df_welfake[['full_text', 'label', 'source']].copy()\n",
    "             print(\"welfake: used only 'text' column as 'full_text'.\")\n",
    "         else:\n",
    "             print(\"error: neither 'title' nor 'text' found in welfake df. cannot proceed with welfake.\")\n",
    "             df_welfake = pd.DataFrame() # make empty\n",
    "\n",
    "# display basic info for loaded dataframes\n",
    "print(\"\\nisot dataset info\")\n",
    "if not df_isot.empty:\n",
    "    df_isot.info()\n",
    "    print(\"\\nisot head:\\n\", df_isot.head())\n",
    "    print(\"\\nisot value counts (0=fake, 1=real):\\n\", df_isot['label'].value_counts())\n",
    "    print(\"\\nisot null values check:\\n\", df_isot.isnull().sum())\n",
    "else:\n",
    "    print(\"isot dataframe is empty or failed to load.\")\n",
    "\n",
    "print(\"\\nwelfake dataset info\")\n",
    "if not df_welfake.empty:\n",
    "    df_welfake.info()\n",
    "    print(\"\\nwelfake head:\\n\", df_welfake.head())\n",
    "    print(\"\\nwelfake value counts (0=fake, 1=real):\\n\", df_welfake['label'].value_counts())\n",
    "    print(\"\\nwelfake null values check:\\n\", df_welfake.isnull().sum())\n",
    "else:\n",
    "    print(\"welfake dataframe is empty or failed to load.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4716737-62e0-4699-884e-7ea06098ac99",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing & EDA\n",
    "\n",
    "The following code block applies further preprocessing and performs Exploratory Data Analysis:\n",
    "\n",
    "1.  **Text Cleaning**: Applies the `clean_text` function to the `full_text` column of both DataFrames to create a `cleaned_text` column.\n",
    "\n",
    "2.  **Text Length Analysis**: Calculates the length of the `cleaned_text` and plots histograms to visualise the distribution of text lengths for both datasets. Descriptive statistics for text lengths are also printed.\n",
    "\n",
    "3.  **Word Clouds**: Generates word clouds for fake and real news within each dataset using the `generate_word_cloud` function. Note: This can be slow on large datasets.\n",
    "\n",
    "4.  **Filtering & Deduplication**: Removes rows with very short or empty `cleaned_text` (e.g., <= 10 characters). Checks for and removes duplicate rows based on `cleaned_text` and `label` to avoid data leakage.\n",
    "\n",
    "5.  **Final Balance Check**: Prints the normalised value counts for the 'label' column in each DataFrame after cleaning and filtering to show the final class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7fff07-e2a0-420e-aec7-89d7fa2df2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply basic cleaning to 'full_text'\n",
    "if not df_isot.empty:\n",
    "    df_isot['cleaned_text'] = df_isot['full_text'].apply(clean_text)\n",
    "    print(\"isot: applied clean_text.\")\n",
    "if not df_welfake.empty:\n",
    "    df_welfake['cleaned_text'] = df_welfake['full_text'].apply(clean_text)\n",
    "    print(\"welfake: applied clean_text.\")\n",
    "\n",
    "# analyse text lengths of cleaned text\n",
    "print(\"\\nanalysing text lengths\")\n",
    "if not df_isot.empty:\n",
    "    df_isot['text_length'] = df_isot['cleaned_text'].apply(len)\n",
    "if not df_welfake.empty:\n",
    "    df_welfake['text_length'] = df_welfake['cleaned_text'].apply(len)\n",
    "\n",
    "# plot histograms of text lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_lengths = False # flag to check if anything was plotted\n",
    "if not df_isot.empty:\n",
    "    sns.histplot(data=df_isot, x='text_length', bins=50, kde=True, label='ISOT', color='blue', alpha=0.6)\n",
    "    print(\"\\nISOT cleaned text length stats:\\n\", df_isot['text_length'].describe())\n",
    "    plot_lengths = True\n",
    "if not df_welfake.empty:\n",
    "    sns.histplot(data=df_welfake, x='text_length', bins=50, kde=True, label='WELFake', color='red', alpha=0.6)\n",
    "    print(\"\\nWELFake cleaned text length stats:\\n\", df_welfake['text_length'].describe())\n",
    "    plot_lengths = True\n",
    "\n",
    "if plot_lengths:\n",
    "    plt.title('Distribution of Text Lengths (Cleaned Text)')\n",
    "    plt.xlabel('Number of Characters')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"no data available to plot text length distributions.\")\n",
    "\n",
    "\n",
    "# word clouds\n",
    "\n",
    "# set to true to generate word clouds (kinda time-consuming and I didn't actually learn anything from it)\n",
    "GENERATE_WORDCLOUDS = False \n",
    "\n",
    "if GENERATE_WORDCLOUDS:\n",
    "    print(\"\\ngenerating word clouds\")\n",
    "    # i might consider sampling if datasets are very large: df.sample(n=10000, random_state=SEED)\n",
    "    if not df_isot.empty:\n",
    "        print(\"generating isot word clouds...\")\n",
    "        generate_word_cloud(df_isot, 0, 'Fake', 'ISOT')\n",
    "        generate_word_cloud(df_isot, 1, 'Real', 'ISOT')\n",
    "    else:\n",
    "        print(\"skipping isot word clouds (dataframe empty).\")\n",
    "\n",
    "    if not df_welfake.empty:\n",
    "        print(\"generating welfake word clouds...\")\n",
    "        generate_word_cloud(df_welfake, 0, 'Fake', 'WELFake')\n",
    "        generate_word_cloud(df_welfake, 1, 'Real', 'WELFake')\n",
    "    else:\n",
    "        print(\"skipping welfake word clouds (dataframe empty).\")\n",
    "else:\n",
    "    print(\"GENERATE_WORDCLOUDS set to False - not generating word clouds for EDA\")\n",
    "\n",
    "\n",
    "# filter short/empty texts, drop duplicates\n",
    "min_text_length = 10 # minimum number of characters required for cleaned text\n",
    "\n",
    "if not df_isot.empty:\n",
    "    initial_len = len(df_isot)\n",
    "    # filter based on cleaned text length\n",
    "    df_isot = df_isot[df_isot['cleaned_text'].str.len() > min_text_length].copy()\n",
    "    print(f\"isot: removed {initial_len - len(df_isot)} articles with cleaned_text length <= {min_text_length}.\")\n",
    "    # drop duplicates based on cleaned text and label\n",
    "    initial_len = len(df_isot)\n",
    "    df_isot = df_isot.drop_duplicates(subset=['cleaned_text', 'label'], keep='first')\n",
    "    print(f\"isot: removed {initial_len - len(df_isot)} duplicate articles (based on cleaned_text & label).\")\n",
    "    print(f\"isot: final shape after filtering/deduplication: {df_isot.shape}\")\n",
    "\n",
    "if not df_welfake.empty:\n",
    "    initial_len = len(df_welfake)\n",
    "    # filter based on cleaned text length\n",
    "    df_welfake = df_welfake[df_welfake['cleaned_text'].str.len() > min_text_length].copy()\n",
    "    print(f\"welfake: removed {initial_len - len(df_welfake)} articles with cleaned_text length <= {min_text_length}.\")\n",
    "    # drop duplicates based on cleaned text and label\n",
    "    initial_len = len(df_welfake)\n",
    "    df_welfake = df_welfake.drop_duplicates(subset=['cleaned_text', 'label'], keep='first')\n",
    "    print(f\"welfake: removed {initial_len - len(df_welfake)} duplicate articles (based on cleaned_text & label).\")\n",
    "    print(f\"welfake: final shape after filtering/deduplication: {df_welfake.shape}\")\n",
    "\n",
    "# final checks on data balance after prepreocessing\n",
    "print(\"\\nfinal data balance after preprocessing\")\n",
    "if not df_isot.empty:\n",
    "    print(\"isot label distribution:\\n\", df_isot['label'].value_counts(normalize=True))\n",
    "else:\n",
    "    print(\"isot dataframe is empty.\")\n",
    "if not df_welfake.empty:\n",
    "    print(\"\\nwelfake label distribution:\\n\", df_welfake['label'].value_counts(normalize=True))\n",
    "else:\n",
    "    print(\"welfake dataframe is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0f373-d6ed-486c-a5a0-4768ee8f8eb1",
   "metadata": {},
   "source": [
    "## 3. Domain Classifier Training (ISOT vs WELFake)\n",
    "\n",
    "The following code block trains a simple auxiliary classifier. Its purpose is to predict whether a given piece of cleaned text is more stylistically similar to the articles found in the ISOT dataset or the WELFake dataset. This domain prediction will be used later in the GUI to select the most appropriate specialized head from the main MTL model for Fake/Real classification.\n",
    "\n",
    "1.  **Data Preparation**: Combines `cleaned_text` from `df_isot` and `df_welfake`, creating a `domain_label` (0 for ISOT, 1 for WELFake).\n",
    "\n",
    "2.  **Splitting**: Splits this combined data for training and testing the domain classifier.\n",
    "\n",
    "3.  **TF-IDF Vectorisation**: Fits a `TfidfVectorizer` specifically for the domain classification task.\n",
    "\n",
    "4.  **Logistic Regression Training**: Trains a `LogisticRegression` model (`domain_classifier`) to predict the `domain_label`.\n",
    "\n",
    "5.  **Evaluation**: Briefly evaluates the accuracy and classification report of the trained domain classifier on its test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284eefc5-af79-490b-b457-3dbcfe38a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain classifier training\n",
    "\n",
    "print(\"\\ntraining domain classifier (tf-idf + logistic regression)\")\n",
    "\n",
    "domain_vectorizer = None\n",
    "domain_classifier = None\n",
    "df_domain_data = pd.DataFrame() # initialize empty dataframe\n",
    "\n",
    "# prepare data for domain classification\n",
    "if not df_isot.empty and 'cleaned_text' in df_isot.columns and \\\n",
    "   not df_welfake.empty and 'cleaned_text' in df_welfake.columns:\n",
    "\n",
    "    print(\"preparing data for domain classification...\")\n",
    "    # select relevant columns and add domain label\n",
    "    df_isot_domain = df_isot[['cleaned_text']].copy()\n",
    "    df_isot_domain['domain_label'] = 0 # 0 for ISOT\n",
    "\n",
    "    df_welfake_domain = df_welfake[['cleaned_text']].copy()\n",
    "    df_welfake_domain['domain_label'] = 1 # 1 for WELFake\n",
    "\n",
    "    # combine data\n",
    "    df_domain_data = pd.concat([df_isot_domain, df_welfake_domain], ignore_index=True)\n",
    "    # shuffle combined data\n",
    "    df_domain_data = df_domain_data.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    print(f\"combined domain dataset size: {len(df_domain_data)}\")\n",
    "    print(\"domain label distribution:\\n\", df_domain_data['domain_label'].value_counts(normalize=True))\n",
    "\n",
    "    # split data for domain classifier training/evaluation\n",
    "    print(\"splitting domain data...\")\n",
    "    try:\n",
    "        domain_train_df, domain_test_df = train_test_split(\n",
    "            df_domain_data,\n",
    "            test_size=0.2, # use 20% for testing the domain classifier\n",
    "            random_state=SEED,\n",
    "            stratify=df_domain_data['domain_label']\n",
    "        )\n",
    "        print(f\"domain split sizes: train={len(domain_train_df)}, test={len(domain_test_df)}\")\n",
    "\n",
    "        # train tf-idf vectoriser for domain\n",
    "        print(\"initialising and fitting domain tf-idf vectoriser...\")\n",
    "        domain_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1, 1)) # simpler vectorizer worked best here\n",
    "        X_domain_train_tfidf = domain_vectorizer.fit_transform(domain_train_df['cleaned_text'])\n",
    "        y_domain_train = domain_train_df['domain_label']\n",
    "        print(f\"domain tf-idf training matrix shape: {X_domain_train_tfidf.shape}\")\n",
    "\n",
    "        # train logistic regression classifier for domain\n",
    "        print(\"training domain logistic regression classifier...\")\n",
    "        domain_classifier = LogisticRegression(solver='liblinear', random_state=SEED, max_iter=1000)\n",
    "        domain_classifier.fit(X_domain_train_tfidf, y_domain_train)\n",
    "        print(\"domain classifier trained.\")\n",
    "\n",
    "        # evaluate domain classifier\n",
    "        print(\"\\nevaluating domain classifier performance...\")\n",
    "        X_domain_test_tfidf = domain_vectorizer.transform(domain_test_df['cleaned_text'])\n",
    "        y_domain_test = domain_test_df['domain_label']\n",
    "        domain_pred_test = domain_classifier.predict(X_domain_test_tfidf)\n",
    "        domain_acc = accuracy_score(y_domain_test, domain_pred_test)\n",
    "        print(f\"  domain classifier test accuracy: {domain_acc:.4f}\")\n",
    "        print(\"  domain classifier classification report:\\n\",\n",
    "              classification_report(y_domain_test, domain_pred_test, target_names=['ISOT (0)', 'WELFake (1)'], digits=4))\n",
    "\n",
    "    except Exception as e_domain:\n",
    "        print(f\"an error occurred during domain classifier training/evaluation: {e_domain}\")\n",
    "        domain_vectorizer = None\n",
    "        domain_classifier = None\n",
    "\n",
    "else:\n",
    "    print(\"skipping domain classifier training: cleaned isot or welfake dataframes are missing/empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61a2ee-abf9-47d9-b308-28374173b18a",
   "metadata": {},
   "source": [
    "## 4. Data Splitting\n",
    "\n",
    "The following code block splits the processed ISOT and WELFake DataFrames into training, validation, and test sets using the `split_data` function.\n",
    "\n",
    "Stratification is used to ensure similar label distributions across the splits. It also creates a combined training set (`df_combined_train`) by concatenating the training sets from both datasets, which is primarily used for training the baseline model, and shuffles it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aedb81-a917-4c7a-a3bd-8b1d61dc668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nsplitting data into train/validation/test sets\")\n",
    "\n",
    "# perform splits for each dataset using the split_data function\n",
    "print(\"splitting isot dataset...\")\n",
    "df_isot_train, df_isot_val, df_isot_test = split_data(df_isot, test_size=0.2, val_size_of_train=0.1, random_state=SEED)\n",
    "\n",
    "print(\"\\nsplitting welfake dataset...\")\n",
    "df_welfake_train, df_welfake_val, df_welfake_test = split_data(df_welfake, test_size=0.2, val_size_of_train=0.1, random_state=SEED)\n",
    "\n",
    "# create combined training set (for baseline model)\n",
    "print(\"\\ncreating combined training set (for baseline)\")\n",
    "# combine only if both individual training sets were successfully created\n",
    "if not df_isot_train.empty and not df_welfake_train.empty:\n",
    "    df_combined_train = pd.concat([df_isot_train, df_welfake_train], ignore_index=True)\n",
    "    print(\"combined isot and welfake training sets.\")\n",
    "elif not df_isot_train.empty:\n",
    "    print(\"using only isot for combined training (welfake train split missing/empty).\")\n",
    "    df_combined_train = df_isot_train.copy()\n",
    "elif not df_welfake_train.empty:\n",
    "    print(\"using only welfake for combined training (isot train split missing/empty).\")\n",
    "    df_combined_train = df_welfake_train.copy()\n",
    "else:\n",
    "    print(\"error: both isot and welfake training sets are empty. cannot create combined set.\")\n",
    "    df_combined_train = pd.DataFrame()\n",
    "\n",
    "# shuffle the combined training data if it was created\n",
    "if not df_combined_train.empty:\n",
    "    df_combined_train = df_combined_train.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    print(f\"combined training set size: {len(df_combined_train)}\")\n",
    "    print(\"combined training set label distribution:\\n\", df_combined_train['label'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\ncreating combined test set...\")\n",
    "# combine only if both individual test sets were successfully created\n",
    "if not df_isot_test.empty and not df_welfake_test.empty:\n",
    "    df_combined_test = pd.concat([df_isot_test, df_welfake_test], ignore_index=True)\n",
    "    print(f\"combined test set created. size: {len(df_combined_test)}\")\n",
    "    # no shuffling needed for test set\n",
    "elif not df_isot_test.empty:\n",
    "    print(\"using only isot for combined test (welfake test split missing/empty).\")\n",
    "    df_combined_test = df_isot_test.copy()\n",
    "    print(f\"combined test set created (isot only). size: {len(df_combined_test)}\")\n",
    "elif not df_welfake_test.empty:\n",
    "    print(\"using only welfake for combined test (isot test split missing/empty).\")\n",
    "    df_combined_test = df_welfake_test.copy()\n",
    "    print(f\"combined test set created (welfake only). size: {len(df_combined_test)}\")\n",
    "else:\n",
    "    print(\"warning: both isot and welfake test sets are empty. cannot create combined test set.\")\n",
    "    df_combined_test = pd.DataFrame()\n",
    "\n",
    "print(\"\\ndata splitting complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7687fb0c-4fc8-4557-ad90-0febf3abf546",
   "metadata": {},
   "source": [
    "## 5. Baseline Model: TF-IDF + Logistic Regression\n",
    "\n",
    "The following code block implements and evaluates a simple baseline model:\n",
    "\n",
    "1.  **TF-IDF Vectorisation**: Initialises a `TfidfVectorizer` (with English stop words, max features, and n-gram range) and fits it *only* on the `df_combined_train` data. It then transforms the combined training data and *both* test sets (ISOT and WELFake) into TF-IDF feature matrices.\n",
    "\n",
    "2.  **Logistic Regression Training**: Initialises and trains a `LogisticRegression` model using the TF-IDF features from the combined training set and corresponding labels.\n",
    "\n",
    "3.  **Baseline Evaluation**: Evaluates the trained baseline model separately on the ISOT test set and the WELFake test set using the `evaluate_predictions` function, which calculates metrics and generates plots (Confusion Matrix, ROC Curve). The results (scalar metrics) are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c33a3-41d2-4964-be8a-98c7e9800b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nbaseline model (tf-idf + logistic regression):\")\n",
    "\n",
    "# initialise dictionary to store baseline evaluation results\n",
    "baseline_results_summary = {\"ISOT\": None, \"WELFake\": None, \"Combined\": None}\n",
    "baseline_model = None # initialise model variable\n",
    "\n",
    "# proceed only if combined training data exists\n",
    "if not df_combined_train.empty:\n",
    "    print(\"initialising tf-idf vectoriser...\")\n",
    "    tfidf_vectoriser = TfidfVectorizer(stop_words='english',\n",
    "                                       max_features=10000, # limit features for efficiency\n",
    "                                       ngram_range=(1, 2)) # include unigrams and bigrams\n",
    "\n",
    "    # fit TF-IDF on training data, and transform all sets\n",
    "    try:\n",
    "        # fit vectoriser on training data only\n",
    "        X_train_tfidf = tfidf_vectoriser.fit_transform(df_combined_train['cleaned_text'])\n",
    "        y_train = df_combined_train['label']\n",
    "        print(f\"tf-idf training matrix shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "        # transform test sets using the *fitted* vectoriser\n",
    "        print(\"transforming test sets...\")\n",
    "        X_isot_test_tfidf = tfidf_vectoriser.transform(df_isot_test['cleaned_text']) if not df_isot_test.empty else None\n",
    "        y_isot_test = df_isot_test['label'].values if not df_isot_test.empty else None # use .values for numpy array\n",
    "\n",
    "        X_welfake_test_tfidf = tfidf_vectoriser.transform(df_welfake_test['cleaned_text']) if not df_welfake_test.empty else None\n",
    "        y_welfake_test = df_welfake_test['label'].values if not df_welfake_test.empty else None # use .values\n",
    "\n",
    "        # train logistic regression model \n",
    "        print(\"training logistic regression model...\")\n",
    "        baseline_model = LogisticRegression(solver='liblinear', # this turned out to be a good solver for this\n",
    "                                           random_state=SEED,\n",
    "                                           C=1.0, # regularisation strength (inverse)\n",
    "                                           max_iter=1000) # i'm prepared to increase max_iter if convergence issues occur\n",
    "        baseline_model.fit(X_train_tfidf, y_train)\n",
    "        print(\"baseline model trained.\")\n",
    "\n",
    "        # evaluate baseline model on test sets\n",
    "        # evaluate on isot test set\n",
    "        if X_isot_test_tfidf is not None and y_isot_test is not None:\n",
    "            print(\"\\nevaluating baseline on isot test set...\")\n",
    "            # get predictions and probabilities\n",
    "            baseline_pred_isot = baseline_model.predict(X_isot_test_tfidf)\n",
    "            baseline_proba_isot = baseline_model.predict_proba(X_isot_test_tfidf)[:, 1] # prob of class 1\n",
    "            # generate report and plots, store scalar metrics\n",
    "            baseline_results_summary[\"ISOT\"] = evaluate_predictions(y_isot_test, baseline_pred_isot, baseline_proba_isot,\n",
    "                                                                    \"Baseline (TF-IDF+LR)\", \"ISOT Test\")\n",
    "        else:\n",
    "            print(\"\\nskipping baseline evaluation on isot test: data missing.\")\n",
    "\n",
    "        # evaluate on welfake test set\n",
    "        if X_welfake_test_tfidf is not None and y_welfake_test is not None:\n",
    "            print(\"\\nevaluating baseline on welfake test set...\")\n",
    "            # get predictions and probabilities\n",
    "            baseline_pred_welfake = baseline_model.predict(X_welfake_test_tfidf)\n",
    "            baseline_proba_welfake = baseline_model.predict_proba(X_welfake_test_tfidf)[:, 1] # prob of class 1\n",
    "            # generate report and plots, store scalar metrics\n",
    "            baseline_results_summary[\"WELFake\"] = evaluate_predictions(y_welfake_test, baseline_pred_welfake, baseline_proba_welfake,\n",
    "                                                                       \"Baseline (TF-IDF+LR)\", \"WELFake Test\")\n",
    "        else:\n",
    "             print(\"\\nskipping baseline evaluation on welfake test: data missing.\")\n",
    "\n",
    "        # evaluate on combined test set\n",
    "        if not df_combined_test.empty and baseline_model is not None and 'tfidf_vectoriser' in locals():\n",
    "            print(\"\\nevaluating baseline on combined test set...\")\n",
    "            try:\n",
    "                # transform combined test set\n",
    "                X_combined_test_tfidf = tfidf_vectoriser.transform(df_combined_test['cleaned_text'])\n",
    "                y_combined_test = df_combined_test['label'].values\n",
    "\n",
    "                # get predictions and probabilities\n",
    "                baseline_pred_combined = baseline_model.predict(X_combined_test_tfidf)\n",
    "                baseline_proba_combined = baseline_model.predict_proba(X_combined_test_tfidf)[:, 1] # prob of class 1\n",
    "\n",
    "                # generate report and plots, store scalar metrics\n",
    "                baseline_results_summary[\"Combined\"] = evaluate_predictions(\n",
    "                    y_combined_test, baseline_pred_combined, baseline_proba_combined,\n",
    "                    \"Baseline (TF-IDF+LR)\", \"Combined Test\"\n",
    "                )\n",
    "            except Exception as e_comb_base:\n",
    "                 print(f\"error evaluating baseline on combined test set: {e_comb_base}\")\n",
    "        else:\n",
    "             print(\"\\nskipping baseline evaluation on combined test: data or model missing.\")\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"an error occurred during baseline model processing: {e}\")\n",
    "        baseline_model = None # ensure model is none if an error occurs\n",
    "\n",
    "else:\n",
    "    print(\"skipping baseline model: combined training data is empty.\")\n",
    "\n",
    "print(\"\\nbaseline model section finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd234935-80f2-472d-8773-6b828371e4fc",
   "metadata": {},
   "source": [
    "## 6. BERT & Multi-Task Learning (MTL) Setup\n",
    "\n",
    "The following code block prepares the data and tokenizer for the DistilBERT Multi-Task Learning model:\n",
    "\n",
    "1.  **Tokenizer Loading**: Loads the pre-trained `DistilBertTokenizer` specified in the configuration (`BERT_MODEL_NAME`). Includes error handling.\n",
    "\n",
    "2.  **Dataset Creation**: Creates `NewsDataset` instances for the training, validation, and test splits of *both* ISOT and WELFake datasets using the `create_dataset` function and the loaded tokenizer. This step is skipped if the tokenizer failed to load.\n",
    "\n",
    "3.  **DataLoader Creation**: Creates PyTorch `DataLoader` instances for all the created datasets using the `create_data_loader` function, setting appropriate batch sizes and shuffling behavior (shuffle=True for training loaders). This step is also skipped if the tokenizer failed to load or datasets weren't created. Checks if all necessary loaders for MTL training are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d8630-460e-4cc2-9344-7cbb8e9ac980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokeniser\n",
    "print(f\"loading distilbert tokeniser: {BERT_MODEL_NAME}\")\n",
    "try:\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "    print(\"tokeniser loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"error loading tokeniser {BERT_MODEL_NAME}: {e}\")\n",
    "    # if tokenizer fails, we cannot proceed with bert models.\n",
    "    tokenizer = None # set to none to prevent downstream errors\n",
    "\n",
    "# initialise loader variables to none\n",
    "isot_train_loader, isot_val_loader, isot_test_loader = None, None, None\n",
    "welfake_train_loader, welfake_val_loader, welfake_test_loader = None, None, None\n",
    "mtl_training_possible = False\n",
    "\n",
    "# proceed only if tokeniser loaded successfully\n",
    "if tokenizer:\n",
    "    # create pytorch datasets\n",
    "    # create datasets for isot\n",
    "    isot_train_dataset = create_dataset(df_isot_train, tokenizer, MAX_LEN)\n",
    "    isot_val_dataset = create_dataset(df_isot_val, tokenizer, MAX_LEN)\n",
    "    isot_test_dataset = create_dataset(df_isot_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    # create datasets for welfake\n",
    "    welfake_train_dataset = create_dataset(df_welfake_train, tokenizer, MAX_LEN)\n",
    "    welfake_val_dataset = create_dataset(df_welfake_val, tokenizer, MAX_LEN)\n",
    "    welfake_test_dataset = create_dataset(df_welfake_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    # create dataloaders\n",
    "    print(\"\\ncreating pytorch dataloaders...\")\n",
    "    num_workers = 0 # number of cpu processes for data loading (0 means main process)\n",
    "\n",
    "    # create dataloaders for isot\n",
    "    print(\"creating isot dataloaders...\")\n",
    "    isot_train_loader = create_data_loader(isot_train_dataset, BATCH_SIZE, num_workers=num_workers, shuffle=True)\n",
    "    isot_val_loader = create_data_loader(isot_val_dataset, BATCH_SIZE, num_workers=num_workers, shuffle=False)\n",
    "    isot_test_loader = create_data_loader(isot_test_dataset, BATCH_SIZE, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    # create dataloaders for welfake\n",
    "    print(\"creating welfake dataloaders...\")\n",
    "    welfake_train_loader = create_data_loader(welfake_train_dataset, BATCH_SIZE, num_workers=num_workers, shuffle=True)\n",
    "    welfake_val_loader = create_data_loader(welfake_val_dataset, BATCH_SIZE, num_workers=num_workers, shuffle=False)\n",
    "    welfake_test_loader = create_data_loader(welfake_test_dataset, BATCH_SIZE, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    # check if essential loaders for mtl training were created successfully\n",
    "    mtl_training_possible = (isot_train_loader is not None and\n",
    "                             welfake_train_loader is not None and\n",
    "                             isot_val_loader is not None and\n",
    "                             welfake_val_loader is not None)\n",
    "    if not mtl_training_possible:\n",
    "         print(\"\\nwarning: not all required dataloaders for mtl training could be created. training will be skipped.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nskipping bert dataset/dataloader creation because tokenizer failed to load.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbf05fc-3980-42fb-acbb-68dcb2008f33",
   "metadata": {},
   "source": [
    "## 7. MTL Model Training\n",
    "\n",
    "The following code block handles the training process for the `DistilBertMTL` model:\n",
    "\n",
    "1.  **Initialisation**: Instantiates the `DistilBertMTL` model and moves it to the selected `device` (GPU or CPU). Skips if setup (tokenizer, loaders) failed previously.\n",
    "   \n",
    "2.  **Optimizer & Scheduler Setup**: Configures the `AdamW` optimizer and a linear learning rate scheduler with warmup (`get_linear_schedule_with_warmup`) based on the total number of training steps (calculated from the combined length of both training loaders and the number of epochs).\n",
    "\n",
    "3.  **Loss Function**: Defines the `CrossEntropyLoss` function.\n",
    "\n",
    "4.  **Training Loop**: Iterates for the specified number of `EPOCHS_MTL`. In each epoch:\n",
    "    *   Calls `train_epoch_mtl` to train the model on interleaved batches from ISOT (Task A) and WELFake (Task B) training loaders.\n",
    "    *   Calls `eval_model_mtl` to evaluate the model's performance on the ISOT validation set (Task A).\n",
    "    *   Calls `eval_model_mtl` to evaluate the model's performance on the WELFake validation set (Task B).\n",
    "    *   Calculates an average validation metric (F1-score) across both tasks.\n",
    "    *   Saves the model's state dictionary (`torch.save`) to `BEST_MTL_MODEL_PATH` if the average validation metric improves compared to the best metric seen so far.\n",
    "    *   Stores training and validation metrics history for plotting later.\n",
    "\n",
    "6.  **Load Best Model**: After the training loop completes, loads the weights of the best-performing model (saved during training based on validation performance) back into the `mtl_model` instance. Includes error handling for file not found or loading issues. Sets the model to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7cea4d-ca35-4be3-90b7-5b1c1c330197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise variables for model and training history\n",
    "mtl_model = None\n",
    "history_mtl = None\n",
    "\n",
    "# dictionary to store final test results (populated in the next section)\n",
    "final_mtl_results_summary = {\"ISOT\": None, \"WELFake\": None, \"Combined\": None}\n",
    "\n",
    "# proceed only if setup was successful (tokeniser loaded, essential dataloaders created)\n",
    "if mtl_training_possible:\n",
    "    print(f\"starting mtl model training for {EPOCHS_MTL} epochs.\")\n",
    "\n",
    "    # initialise model! \n",
    "    mtl_model = DistilBertMTL(bert_model_name=BERT_MODEL_NAME)\n",
    "    mtl_model.to(device)\n",
    "\n",
    "    # optimiser & scheduler\n",
    "    # calculate total training steps needed for scheduler configuration\n",
    "    total_steps_mtl = (len(isot_train_loader) + len(welfake_train_loader)) * EPOCHS_MTL\n",
    "    optimizer_mtl = torch.optim.AdamW(mtl_model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
    "    scheduler_mtl = get_linear_schedule_with_warmup(\n",
    "        optimizer_mtl,\n",
    "        num_warmup_steps=0, # set number of warmup steps (e.g., 10% of total steps) if desired\n",
    "        num_training_steps=total_steps_mtl\n",
    "    )\n",
    "    print(f\"optimiser and scheduler configured for {total_steps_mtl} total steps.\")\n",
    "\n",
    "    # loss function \n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # training loop\n",
    "    best_avg_val_f1 = 0.0 # track best average validation f1 score across tasks\n",
    "    # initialise history dictionary to store metrics per epoch\n",
    "    history_mtl = {'train_loss': [], 'train_acc': [],\n",
    "                   'val_loss_A': [], 'val_acc_A': [], 'val_f1_A': [],\n",
    "                   'val_loss_B': [], 'val_acc_B': [], 'val_f1_B': [],\n",
    "                   'avg_val_f1': []}\n",
    "\n",
    "    # loop over the specified number of epochs\n",
    "    for epoch in range(EPOCHS_MTL):\n",
    "        print(f'\\nepoch {epoch + 1}/{EPOCHS_MTL}')\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # training phase for current epoch)\n",
    "        # call the training function for one epoch\n",
    "        try:\n",
    "            train_loss, train_acc = train_epoch_mtl(\n",
    "                mtl_model, isot_train_loader, welfake_train_loader, loss_fn, optimizer_mtl, device, scheduler_mtl, epoch_num=epoch + 1, total_epochs=EPOCHS_MTL\n",
    "            )\n",
    "        except Exception as e_call: \n",
    "            break # Stop training if the call fails\n",
    "\n",
    "        # store training metrics\n",
    "        history_mtl['train_loss'].append(train_loss)\n",
    "        history_mtl['train_acc'].append(train_acc)\n",
    "        print(f'  epoch {epoch + 1} training complete - avg loss: {train_loss:.4f}, avg acc: {train_acc:.4f}')\n",
    "\n",
    "        # validation phase for current epoch. \n",
    "        print('\\nvalidating mtl...')\n",
    "        # evaluate on task a (isot) validation set\n",
    "        val_metrics_A = eval_model_mtl(mtl_model, isot_val_loader, 'A', loss_fn, device)\n",
    "        # evaluate on task b (welfake) validation set\n",
    "        val_metrics_B = eval_model_mtl(mtl_model, welfake_val_loader, 'B', loss_fn, device)\n",
    "\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "\n",
    "        # process and store validation results\n",
    "        if val_metrics_A and val_metrics_B:\n",
    "            # extract key metrics\n",
    "            val_loss_A, val_acc_A, val_f1_A = val_metrics_A['loss'], val_metrics_A['accuracy'], val_metrics_A['f1']\n",
    "            val_loss_B, val_acc_B, val_f1_B = val_metrics_B['loss'], val_metrics_B['accuracy'], val_metrics_B['f1']\n",
    "            # calculate average f1 across tasks for model saving criterion\n",
    "            avg_val_f1 = (val_f1_A + val_f1_B) / 2.0\n",
    "\n",
    "            # store validation metrics in history\n",
    "            history_mtl['val_loss_A'].append(val_loss_A); history_mtl['val_acc_A'].append(val_acc_A); history_mtl['val_f1_A'].append(val_f1_A)\n",
    "            history_mtl['val_loss_B'].append(val_loss_B); history_mtl['val_acc_B'].append(val_acc_B); history_mtl['val_f1_B'].append(val_f1_B)\n",
    "            history_mtl['avg_val_f1'].append(avg_val_f1)\n",
    "\n",
    "            # print validation summary for the epoch\n",
    "            print(f'isot val loss: {val_loss_A:.4f}, acc: {val_acc_A:.4f}, f1: {val_f1_A:.4f}')\n",
    "            print(f'welfake val loss: {val_loss_B:.4f}, acc: {val_acc_B:.4f}, f1: {val_f1_B:.4f}')\n",
    "            print(f'=> average val f1: {avg_val_f1:.4f}')\n",
    "\n",
    "            # save best model checkpoint\n",
    "            # save model if average validation f1 improves\n",
    "            if avg_val_f1 > best_avg_val_f1:\n",
    "                print(f\"average validation f1 improved ({best_avg_val_f1:.4f} --> {avg_val_f1:.4f}). saving model checkpoint...\")\n",
    "                torch.save(mtl_model.state_dict(), BEST_MTL_MODEL_PATH)\n",
    "                best_avg_val_f1 = avg_val_f1\n",
    "            else:\n",
    "                print(f\"average validation f1 did not improve from {best_avg_val_f1:.4f}.\")\n",
    "\n",
    "        else:\n",
    "             # handle case where validation metrics couldn't be calculated\n",
    "             print(\"warning: could not get validation metrics for one or both tasks this epoch.\")\n",
    "             # append nan to history to maintain length consistency for plotting\n",
    "             for key in history_mtl.keys():\n",
    "                 if key not in ['train_loss', 'train_acc']: # only for validation metrics\n",
    "                     history_mtl[key].append(np.nan)\n",
    "\n",
    "        print(f'epoch {epoch + 1} time: {epoch_duration:.2f}s')\n",
    "\n",
    "\n",
    "    print(\"\\ntraining of mtl model finished. yay!\")\n",
    "\n",
    "    # load the best model weights found during training\n",
    "    print(f\"loading best model weights from: {BEST_MTL_MODEL_PATH}\")\n",
    "    try:\n",
    "        # ensure model structure exists (in case loop was skipped but file exists)\n",
    "        if mtl_model is None:\n",
    "             mtl_model = DistilBertMTL(bert_model_name=BERT_MODEL_NAME).to(device)\n",
    "        # load the saved state dictionary\n",
    "        mtl_model.load_state_dict(torch.load(BEST_MTL_MODEL_PATH, map_location=device))\n",
    "        print(\"best mtl model weights loaded successfully.\")\n",
    "        # set the model to evaluation mode after loading weights\n",
    "        mtl_model.eval()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"warning: best model checkpoint not found at {BEST_MTL_MODEL_PATH}. using the model's state from the last epoch.\")\n",
    "        # ensure the model is in eval mode if training finished but file wasn't saved/found\n",
    "        if mtl_model: mtl_model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"error loading best mtl model weights: {e}. using the model's state from the last epoch.\")\n",
    "        # ensure the model is in eval mode\n",
    "        if mtl_model: mtl_model.eval()\n",
    "\n",
    "else:\n",
    "    print(\"skipping mtl training: required dataloaders or tokeniser not available.\")\n",
    "    mtl_model = None # ensure model is none if training was skipped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b7dd01-5fe3-4ff4-87f4-f0101b6e8744",
   "metadata": {},
   "source": [
    "## 8. MTL Model Evaluation\n",
    "\n",
    "The following code block evaluates the final trained (or best loaded) `DistilBertMTL` model on the held-out test sets:\n",
    "\n",
    "1.  **Evaluate on ISOT Test Set**: Calls `eval_model_mtl` with the ISOT test loader (`isot_test_loader`) and `task_id='A'`. This returns a dictionary including metrics, predictions, and probabilities. Skips if the model or loader is unavailable.\n",
    "\n",
    "2.  **Report ISOT Test Results**: If evaluation was successful, it calls `evaluate_predictions` using the true labels, predicted labels, and probabilities from the previous step to print metrics and display plots (Confusion Matrix, ROC Curve) specifically for the MTL model's performance on the ISOT test data. The scalar metrics are stored.\n",
    "\n",
    "3.  **Evaluate on WELFake Test Set**: Calls `eval_model_mtl` with the WELFake test loader (`welfake_test_loader`) and `task_id='B'`. Skips if the model or loader is unavailable.\n",
    "\n",
    "4.  **Report WELFake Test Results**: If evaluation was successful, it calls `evaluate_predictions` using the results from the WELFake evaluation to report metrics and plots for the MTL model on the WELFake test data. The scalar metrics are stored.\n",
    "\n",
    "5.  **Plot Training History**: If training was performed (`history_mtl` exists), it plots the training and validation loss, accuracy, and F1 scores over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15313c2-7c2f-4429-a0b9-44f23143cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nevaluating final mtl model on test sets.\")\n",
    "\n",
    "# variables to store raw evaluation results\n",
    "y_true_isot_test, y_pred_isot_test, y_proba_isot_test = None, None, None\n",
    "y_true_welfake_test, y_pred_welfake_test, y_proba_welfake_test = None, None, None\n",
    "\n",
    "\n",
    "# proceed only if the mtl model was trained/loaded successfully and loss_fn exists\n",
    "if mtl_model is not None and 'loss_fn' in locals() and loss_fn is not None:\n",
    "\n",
    "    # evaluate on ISOT test set (Task A)\n",
    "    if isot_test_loader:\n",
    "        print(\"\\nevaluating mtl on isot test set (task a)...\")\n",
    "        # get full evaluation metrics including predictions/probabilities for detailed reporting\n",
    "        eval_results_isot_test = eval_model_mtl(\n",
    "            mtl_model, isot_test_loader, 'A', loss_fn, device\n",
    "        )\n",
    "\n",
    "        if eval_results_isot_test:\n",
    "             # store raw results as numpy arrays \n",
    "             y_true_isot_test = np.array(eval_results_isot_test['true_labels'])\n",
    "             y_pred_isot_test = np.array(eval_results_isot_test['predictions'])\n",
    "             y_proba_isot_test = np.array(eval_results_isot_test['probabilities'])\n",
    "\n",
    "             # generate summary report and plots using evaluate_predictions\n",
    "             # store the returned scalar metrics\n",
    "             final_mtl_results_summary[\"ISOT\"] = evaluate_predictions(\n",
    "                 y_true_isot_test, y_pred_isot_test, y_proba_isot_test, # Use stored arrays\n",
    "                 \"MTL Model\", \"ISOT Test\"\n",
    "             )\n",
    "             if final_mtl_results_summary[\"ISOT\"]:\n",
    "                 print(\"mtl evaluation on isot test complete.\")\n",
    "             else:\n",
    "                 print(\"error generating final report for mtl on isot test.\")\n",
    "        else:\n",
    "             print(\"evaluation failed for mtl on isot test (eval_model_mtl returned none).\")\n",
    "    else:\n",
    "        print(\"skipping mtl evaluation on isot test: loader missing.\")\n",
    "\n",
    "\n",
    "    # evaluate on WELFake test set (Task B) \n",
    "    if welfake_test_loader:\n",
    "        print(\"\\nevaluating mtl on welfake test set (task b)...\")\n",
    "        # get full evaluation metrics\n",
    "        eval_results_welfake_test = eval_model_mtl(\n",
    "            mtl_model, welfake_test_loader, 'B', loss_fn, device\n",
    "        )\n",
    "\n",
    "        if eval_results_welfake_test:\n",
    "            # store raw results as numpy arrays \n",
    "            y_true_welfake_test = np.array(eval_results_welfake_test['true_labels'])\n",
    "            y_pred_welfake_test = np.array(eval_results_welfake_test['predictions'])\n",
    "            y_proba_welfake_test = np.array(eval_results_welfake_test['probabilities'])\n",
    "\n",
    "            # generate summary report and plots, store scalar metrics\n",
    "            final_mtl_results_summary[\"WELFake\"] = evaluate_predictions(\n",
    "                y_true_welfake_test, y_pred_welfake_test, y_proba_welfake_test, # Use stored arrays\n",
    "                \"MTL Model\", \"WELFake Test\"\n",
    "            )\n",
    "            if final_mtl_results_summary[\"WELFake\"]:\n",
    "                 print(\"mtl evaluation on welfake test complete.\")\n",
    "            else:\n",
    "                 print(\"error generating final report for mtl on welfake test.\")\n",
    "        else:\n",
    "             print(\"evaluation failed for mtl on welfake test (eval_model_mtl returned none).\")\n",
    "    else:\n",
    "        print(\"skipping mtl evaluation on welfake test: loader missing.\")\n",
    "\n",
    "    # evaluate on combined test set (using concatenated results)\n",
    "    print(\"\\ncalculating mtl performance on combined test set...\")\n",
    "    # check if results from both individual evaluations are available\n",
    "    if y_true_isot_test is not None and y_true_welfake_test is not None:\n",
    "        # ensure all parts (true, pred, proba) are available for both\n",
    "        if y_pred_isot_test is not None and y_proba_isot_test is not None and \\\n",
    "           y_pred_welfake_test is not None and y_proba_welfake_test is not None:\n",
    "\n",
    "            try: # try-except for concatenation\n",
    "                y_true_combined = np.concatenate((y_true_isot_test, y_true_welfake_test))\n",
    "                y_pred_combined = np.concatenate((y_pred_isot_test, y_pred_welfake_test))\n",
    "                y_proba_combined = np.concatenate((y_proba_isot_test, y_proba_welfake_test))\n",
    "\n",
    "                print(f\"combined test shapes: true={y_true_combined.shape}, pred={y_pred_combined.shape}, proba={y_proba_combined.shape}\")\n",
    "\n",
    "                # generate summary report and plots, store scalar metrics\n",
    "                final_mtl_results_summary[\"Combined\"] = evaluate_predictions(\n",
    "                    y_true_combined, y_pred_combined, y_proba_combined,\n",
    "                    \"MTL Model\", \"Combined Test\"\n",
    "                )\n",
    "                if final_mtl_results_summary[\"Combined\"]:\n",
    "                     print(\"mtl evaluation on combined test complete.\")\n",
    "                else:\n",
    "                     print(\"error generating final report for mtl on combined test.\")\n",
    "            except ValueError as e_concat:\n",
    "                 print(f\"error concatenating results for combined mtl evaluation: {e_concat}\")\n",
    "                 print(\"check shapes of individual result arrays:\")\n",
    "                 print(f\"  isot: true={y_true_isot_test.shape}, pred={y_pred_isot_test.shape}, proba={y_proba_isot_test.shape}\")\n",
    "                 print(f\"  welfake: true={y_true_welfake_test.shape}, pred={y_pred_welfake_test.shape}, proba={y_proba_welfake_test.shape}\")\n",
    "\n",
    "        else:\n",
    "             print(\"skipping mtl combined evaluation: missing prediction/probability arrays from individual tasks.\")\n",
    "    else:\n",
    "        print(\"skipping mtl combined evaluation: missing true label arrays from individual tasks.\")\n",
    "\n",
    "else:\n",
    "    print(\"skipping mtl evaluation: model was not trained or loaded successfully, or loss_fn is missing.\")\n",
    "\n",
    "print(\"\\nmtl model evaluation section finished.\") # Added print\n",
    "\n",
    "# plot training history\n",
    "# plot if training was performed and history was recorded\n",
    "if history_mtl:\n",
    "    print(\"\\nplotting training history\")\n",
    "    try:\n",
    "        # use actual length in case training stopped early or had issues\n",
    "        epochs_with_data = len(history_mtl['train_loss'])\n",
    "        if epochs_with_data == 0:\n",
    "             print(\"no training data recorded in history.\")\n",
    "        else:\n",
    "            epochs_range = range(1, epochs_with_data + 1)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "\n",
    "            # check if validation keys have data before plotting\n",
    "            val_epochs_with_data = len(history_mtl.get('avg_val_f1', []))\n",
    "\n",
    "            # subplot 1: Loss\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.plot(epochs_range, history_mtl['train_loss'][:epochs_with_data], label='Training Loss', marker='.')\n",
    "            if 'val_loss_A' in history_mtl and len(history_mtl['val_loss_A']) >= epochs_with_data:\n",
    "                 plt.plot(epochs_range, history_mtl['val_loss_A'][:epochs_with_data], label='ISOT Validation Loss', linestyle='--', marker='.')\n",
    "            if 'val_loss_B' in history_mtl and len(history_mtl['val_loss_B']) >= epochs_with_data:\n",
    "                 plt.plot(epochs_range, history_mtl['val_loss_B'][:epochs_with_data], label='WELFake Validation Loss', linestyle=':', marker='.')\n",
    "            plt.title('Training and Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "            # subplot 2: accuracy\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.plot(epochs_range, history_mtl['train_acc'][:epochs_with_data], label='Train Accuracy', marker='.')\n",
    "            if 'val_acc_A' in history_mtl and len(history_mtl['val_acc_A']) >= epochs_with_data:\n",
    "                 plt.plot(epochs_range, history_mtl['val_acc_A'][:epochs_with_data], label='ISOT Validation Accuracy', linestyle='--', marker='.')\n",
    "            if 'val_acc_B' in history_mtl and len(history_mtl['val_acc_B']) >= epochs_with_data:\n",
    "                 plt.plot(epochs_range, history_mtl['val_acc_B'][:epochs_with_data], label='WELFake Validation Accuracy', linestyle=':', marker='.')\n",
    "            plt.title('Training and Validation Accuracy')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "            # subplot 3: F1 score\n",
    "            plt.subplot(2, 2, 3)\n",
    "            if 'val_f1_A' in history_mtl and len(history_mtl['val_f1_A']) >= epochs_with_data:\n",
    "                 plt.plot(epochs_range, history_mtl['val_f1_A'][:epochs_with_data], label='ISOT Validation F1', linestyle='--', marker='.')\n",
    "            if 'val_f1_B' in history_mtl and len(history_mtl['val_f1_B']) >= epochs_with_data:\n",
    "                 plt.plot(epochs_range, history_mtl['val_f1_B'][:epochs_with_data], label='WELFake Validation F1', linestyle=':', marker='.')\n",
    "            if 'avg_val_f1' in history_mtl and len(history_mtl['avg_val_f1']) >= epochs_with_data:\n",
    "                 plt.plot(epochs_range, history_mtl['avg_val_f1'][:epochs_with_data], label='Average Validation F1', linestyle='-', marker='o', linewidth=2)\n",
    "            plt.title('Validation F1 Scores')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('F1 Score')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "            # adjust layout to prevent overlapping titles/labels\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"could not plot training history: {e}\")\n",
    "        # this might happen if training was interrupted or history keys have inconsistent lengths or NaN values.\n",
    "else:\n",
    "    print(\"\\nno training history available to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ac2d0-7364-42a1-a87c-3e4b8a96de5d",
   "metadata": {},
   "source": [
    "## 9. Results Summary\n",
    "\n",
    "The following code block aggregates the final evaluation metrics (Accuracy, Precision, Recall, F1, ROC AUC) from both the Baseline model and the MTL model for both test sets (ISOT and WELFake) into a single Pandas DataFrame for easy comparison. The DataFrame is then sorted and printed. Optionally, the summary table can be saved to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51317ab-b8b9-4a54-b96e-e290e98baef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nfinal performance summary:\")\n",
    "\n",
    "# aggregate results into a list of dictionaries\n",
    "results_list = []\n",
    "\n",
    "# add baseline results (checking if they were successfully computed)\n",
    "if baseline_results_summary.get(\"ISOT\"): # use .get() for safer access\n",
    "    results_list.append({\n",
    "        'Dataset': 'ISOT Test',\n",
    "        'Model': 'Baseline (TF-IDF+LR)',\n",
    "        **baseline_results_summary[\"ISOT\"] # unpack the metrics dict\n",
    "    })\n",
    "if baseline_results_summary.get(\"WELFake\"):\n",
    "    results_list.append({\n",
    "        'Dataset': 'WELFake Test',\n",
    "        'Model': 'Baseline (TF-IDF+LR)',\n",
    "        **baseline_results_summary[\"WELFake\"]\n",
    "    })\n",
    "if baseline_results_summary.get(\"Combined\"):\n",
    "    results_list.append({\n",
    "        'Dataset': 'Combined Test', 'Model': 'Baseline (TF-IDF+LR)', **baseline_results_summary[\"Combined\"]\n",
    "    })\n",
    "\n",
    "# add mtl results (checking if they were successfully computed)\n",
    "if final_mtl_results_summary.get(\"ISOT\"):\n",
    "    results_list.append({\n",
    "        'Dataset': 'ISOT Test',\n",
    "        'Model': 'MTL-BERT',\n",
    "        **final_mtl_results_summary[\"ISOT\"]\n",
    "    })\n",
    "if final_mtl_results_summary.get(\"WELFake\"):\n",
    "    results_list.append({\n",
    "        'Dataset': 'WELFake Test',\n",
    "        'Model': 'MTL-BERT',\n",
    "        **final_mtl_results_summary[\"WELFake\"]\n",
    "    })\n",
    "if final_mtl_results_summary.get(\"Combined\"):\n",
    "    results_list.append({\n",
    "        'Dataset': 'Combined Test', 'Model': 'MTL-BERT', **final_mtl_results_summary[\"Combined\"]\n",
    "    })\n",
    "\n",
    "# create and display dataframe.\n",
    "if results_list:\n",
    "    # create dataframe from the list of results\n",
    "    df_results_final = pd.DataFrame(results_list)\n",
    "\n",
    "    # define columns to display and their desired order\n",
    "    display_cols = ['Dataset', 'Model', 'accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    # ensure only available columns are selected (in case some metric calculation failed)\n",
    "    available_cols = [col for col in display_cols if col in df_results_final.columns]\n",
    "\n",
    "    # sort the dataframe for consistent presentation\n",
    "    df_results_sorted = df_results_final.sort_values(by=['Dataset', 'Model']).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\ncomparison of model performance on test sets:\")\n",
    "    display(df_results_sorted[available_cols])\n",
    "\n",
    "    # save the results table to a csv file (or i'll comment this out to not, but feels like i'll need for report)\n",
    "    results_csv_path = os.path.join(RESULTS_DIR, 'final_results_summary.csv')\n",
    "    try:\n",
    "        df_results_sorted[available_cols].to_csv(results_csv_path, index=False, float_format='%.4f') # keep float format for CSV\n",
    "        print(f\"\\nresults summary saved to: {results_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nerror saving results summary to csv: {e}\")\n",
    "\n",
    "else:\n",
    "     print(\"no evaluation results available to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf22e3b-1924-4547-861d-272e9815b1c8",
   "metadata": {},
   "source": [
    "## 10. Interactive GUI for Prediction and Explanation (Domain-Specific + LIME)\n",
    "\n",
    "The following code block sets up and launches an interactive Gradio web interface. This GUI incorporates the domain classification strategy and LIME explanations.\n",
    "\n",
    "1.  **Domain Prediction**: It uses the trained `domain_classifier` to predict the source domain (ISOT/WELFake) of the input text.\n",
    "\n",
    "2.  **Specialized Head Selection**: It selects the appropriate MTL head (A or B) based on the predicted domain.\n",
    "\n",
    "3.  **Final Classification**: It performs Fake/Real classification using the selected head.\n",
    "    \n",
    "4.  **LIME Explanation**: It generates a LIME explanation, highlighting words that contributed to the prediction made by the *selected* head.\n",
    "\n",
    "5.  **Interface**: It uses `gradio` to display the input text box, the final prediction, confidence, text explanation (including domain info), and the LIME visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42619c81-a91a-4211-8758-977f83e916b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive GUI (Application Layer)\n",
    "\n",
    "# this function serves as the predictor required by LIME. it takes a list of\n",
    "# perturbed text samples generated by LIME and returns the model's prediction\n",
    "# probabilities (for fake/real) for each sample using a specific MTL head.\n",
    "# arguments:\n",
    "#   texts - list of text strings (original and perturbed) from LIME\n",
    "#   model - the pytorch mtl model object\n",
    "#   tokenizer - the bert tokenizer instance\n",
    "#   device - the device to run inference on ('cuda' or 'cpu')\n",
    "#   max_len - maximum sequence length for tokenization\n",
    "#   task_id_for_lime - the specific mtl head ('A' or 'B') to use for predictions\n",
    "# returns:\n",
    "#   numpy array of shape (num_samples, 2) with probabilities for [fake, real]\n",
    "def lime_predictor_for_mtl(texts, model, tokenizer, device, max_len, task_id_for_lime):\n",
    "    # step 1: set model to evaluation mode and move to device\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    all_probabilities = []\n",
    "\n",
    "    # step 2: iterate through text samples provided by LIME\n",
    "    for text in texts:\n",
    "        # step 2a: handle potentially empty/invalid samples from LIME perturbations\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            all_probabilities.append(np.array([0.5, 0.5])) # return neutral probability\n",
    "            continue\n",
    "        # step 2b: process valid text samples\n",
    "        try:\n",
    "            # tokenize the text sample\n",
    "            enc = tokenizer.encode_plus(\n",
    "                text, add_special_tokens=True, max_length=max_len,\n",
    "                padding='max_length', truncation=True, return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            ids = enc['input_ids'].to(device)\n",
    "            mask = enc['attention_mask'].to(device)\n",
    "\n",
    "            # perform inference without calculating gradients\n",
    "            with torch.no_grad():\n",
    "                # forward pass using the *specified* task_id for this LIME run\n",
    "                lgts = model(input_ids=ids, attention_mask=mask, task_id=task_id_for_lime)\n",
    "\n",
    "            # apply softmax to get probabilities and move to cpu\n",
    "            prbs = F.softmax(lgts, dim=1).cpu().numpy()[0]\n",
    "            all_probabilities.append(prbs)\n",
    "        except Exception as e_inner_pred:\n",
    "            # handle errors during tokenization or prediction for a specific sample\n",
    "            print(f\"[LIME Predictor] error processing text: '{text[:50]}...' -> {e_inner_pred}\")\n",
    "            all_probabilities.append(np.array([0.5, 0.5])) # return neutral on error\n",
    "\n",
    "    # step 3: convert list of probabilities to numpy array\n",
    "    final_probs = np.array(all_probabilities)\n",
    "\n",
    "    # step 4: validate and potentially correct the output shape for LIME\n",
    "    # LIME expects shape (num_samples, num_classes), e.g., (N, 2)\n",
    "    if final_probs.ndim == 1 and len(texts) == 1 and final_probs.shape[0] == 2:\n",
    "        final_probs = final_probs.reshape(1, 2) # correct shape for single sample case\n",
    "    elif final_probs.ndim != 2 or final_probs.shape[1] != 2:\n",
    "         # if shape is incorrect (e.g., flattened or wrong class count), log warning and return neutral\n",
    "         print(f\"[LIME Predictor] warning: correcting output shape. original: {final_probs.shape}, num texts: {len(texts)}\")\n",
    "         final_probs = np.full((len(texts), 2), 0.5)\n",
    "\n",
    "    # print(f\"[LIME Predictor] Returning probabilities shape: {final_probs.shape}\") # debug print\n",
    "    return final_probs\n",
    "\n",
    "\n",
    "\n",
    "# this function orchestrates the prediction process: cleaning, domain prediction,\n",
    "# mtl prediction using the selected head, and LIME explanation generation.\n",
    "# it is called by the gradio_interface_fn wrapper.\n",
    "# arguments:\n",
    "#   raw_text - the raw input text string from the user\n",
    "#   mtl_model_obj - the trained pytorch mtl model object\n",
    "#   bert_tokenizer - the loaded bert tokenizer instance\n",
    "#   clean_fn - the text cleaning function (e.g., clean_text)\n",
    "#   max_len_val - the maximum sequence length for bert tokenization\n",
    "#   dev - the device ('cuda' or 'cpu')\n",
    "#   domain_clf - the trained domain classifier (e.g., logistic regression)\n",
    "#   domain_vec - the fitted domain tf-idf vectorizer\n",
    "#   num_features_lime - number of features for LIME explanation\n",
    "#   num_samples_lime - number of samples for LIME perturbation\n",
    "# returns:\n",
    "#   list containing [prediction, confidence, text_explanation, details, lime_html_output]\n",
    "def predict_domain_then_classify_logic(raw_text,\n",
    "                                       mtl_model_obj, bert_tokenizer, clean_fn, max_len_val, dev,\n",
    "                                       domain_clf, domain_vec,\n",
    "                                       num_features_lime=10, num_samples_lime=500):\n",
    "\n",
    "    # step 1: initialize output list with default error values\n",
    "    outputs = [\"Error\"] * 5 # prediction, confidence, explanation, details, lime_html\n",
    "    outputs[4] = \"<p style='color:red;'>LIME: Not Started.</p>\" # initial LIME html state\n",
    "\n",
    "    try:\n",
    "        # step 2: basic checks for required components\n",
    "        if mtl_model_obj is None or bert_tokenizer is None or clean_fn is None or \\\n",
    "           domain_clf is None or domain_vec is None:\n",
    "            outputs[2] = \"error: one or more required backend components missing.\"\n",
    "            print(outputs[2])\n",
    "            outputs[4] = \"<p style='color:red;'>LIME Skipped: required component missing.</p>\"\n",
    "            return outputs\n",
    "\n",
    "        # step 3: clean the input text\n",
    "        cleaned = clean_fn(raw_text)\n",
    "        # step 3a: handle case where cleaning results in empty text\n",
    "        if not cleaned.strip():\n",
    "            outputs[2] = \"error: Cleaned text is empty or whitespace only.\"\n",
    "            print(outputs[2])\n",
    "            outputs[4] = \"<p style='color:orange;'>LIME skipped: input text is empty after cleaning.</p>\"\n",
    "            return outputs\n",
    "\n",
    "        # step 4: predict the domain (ISOT or WELFake)\n",
    "        predicted_domain_name = \"Unknown\"\n",
    "        selected_task_id = None\n",
    "        domain_pred_proba = [0.0, 0.0]\n",
    "        domain_confidence = 0.0\n",
    "        try:\n",
    "            # step 4a: vectorize cleaned text using domain vectorizer\n",
    "            text_tfidf_domain = domain_vec.transform([cleaned])\n",
    "            # step 4b: get domain prediction probabilities\n",
    "            domain_pred_proba = domain_clf.predict_proba(text_tfidf_domain)[0] # [P_ISOT, P_WELFake]\n",
    "            # step 4c: determine predicted domain label and confidence\n",
    "            predicted_domain_label = np.argmax(domain_pred_proba)\n",
    "            domain_confidence = np.max(domain_pred_proba)\n",
    "            # step 4d: map label to task_id ('A' or 'B') and name\n",
    "            selected_task_id = 'A' if predicted_domain_label == 0 else 'B'\n",
    "            predicted_domain_name = \"ISOT\" if selected_task_id == 'A' else \"WELFake\"\n",
    "        except Exception as e_domain_pred:\n",
    "            # handle errors during domain prediction\n",
    "            outputs[2] = f\"error during domain prediction: {e_domain_pred}\"\n",
    "            print(outputs[2])\n",
    "            outputs[4] = f\"<p style='color:red;'>LIME skipped due to domain prediction error: {e_domain_pred}</p>\"\n",
    "            return outputs # return early if domain prediction fails\n",
    "\n",
    "        # step 5: get fake/real prediction from the selected MTL head\n",
    "        prediction_final = \"Error\"\n",
    "        confidence_final_str = \"0.00%\"\n",
    "        logits = None\n",
    "        probs = np.array([0.5, 0.5]) # default probabilities\n",
    "        try:\n",
    "            # step 5a: set model to evaluation mode and move to device\n",
    "            mtl_model_obj = mtl_model_obj.to(dev)\n",
    "            mtl_model_obj.eval()\n",
    "            # step 5b: tokenize cleaned text for BERT\n",
    "            encoding = bert_tokenizer.encode_plus(\n",
    "                cleaned, add_special_tokens=True, max_length=max_len_val,\n",
    "                padding='max_length', truncation=True, return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            input_ids = encoding['input_ids'].to(dev)\n",
    "            attention_mask = encoding['attention_mask'].to(dev)\n",
    "            # step 5c: forward pass through MTL model using selected head\n",
    "            with torch.no_grad():\n",
    "                logits = mtl_model_obj(input_ids=input_ids, attention_mask=attention_mask, task_id=selected_task_id)\n",
    "            # step 5d: calculate probabilities\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()[0] # [P_fake, P_real]\n",
    "            fake_prob, real_prob = probs[0], probs[1]\n",
    "            # step 5e: determine final prediction and confidence\n",
    "            prediction_final = \"REAL\" if real_prob >= fake_prob else \"FAKE\"\n",
    "            confidence_final = real_prob if prediction_final == \"REAL\" else fake_prob\n",
    "            confidence_final_str = f\"{confidence_final:.2%}\"\n",
    "        except Exception as e_mtl_pred:\n",
    "             # handle errors during MTL prediction\n",
    "             outputs[2] = f\"error during MTL prediction (Head {selected_task_id}): {e_mtl_pred}\"\n",
    "             print(outputs[2])\n",
    "             outputs[4] = f\"<p style='color:red;'>LIME skipped due to MTL prediction error: {e_mtl_pred}</p>\"\n",
    "             return outputs # return early if MTL prediction fails\n",
    "\n",
    "        # LIME explanation generation \n",
    "        lime_html = \"<p style='color:red;'>LIME: failed inside try block.</p>\" # default error message\n",
    "        try:\n",
    "            # step 6a: create a partial function for the LIME predictor, fixing the\n",
    "            # arguments needed by lime_predictor_for_mtl for this specific run\n",
    "            lime_predictor_partial = functools.partial(\n",
    "                lime_predictor_for_mtl, # use the external function defined above\n",
    "                model=mtl_model_obj,\n",
    "                tokenizer=bert_tokenizer,\n",
    "                device=dev,\n",
    "                max_len=max_len_val,\n",
    "                task_id_for_lime=selected_task_id # Pass the dynamically selected task ID\n",
    "            )\n",
    "\n",
    "            # step 6b: instantiate the LIME text explainer\n",
    "            explainer = lime.lime_text.LimeTextExplainer(class_names=[\"FAKE\", \"REAL\"])\n",
    "\n",
    "            # step 6c: generate the explanation object from LIME\n",
    "            explanation_lime = explainer.explain_instance(\n",
    "                cleaned,                 # the text instance to explain\n",
    "                lime_predictor_partial,  # the prediction function (with fixed args)\n",
    "                num_features=num_features_lime, # how many words to show\n",
    "                num_samples=num_samples_lime    # number of perturbations for LIME\n",
    "            )\n",
    "\n",
    "            # step 6d: format the LIME explanation as HTML (using iframe for large content)\n",
    "            if explanation_lime:\n",
    "                lime_html_content = explanation_lime.as_html()\n",
    "                # check if the generated html is very short (might indicate an issue)\n",
    "                if len(lime_html_content) < 100:\n",
    "                     lime_html = \"<p style='color:orange;'>LIME: explanation returned empty or very short.</p>\"\n",
    "                else:\n",
    "                    # format using iframe and base64 encoding to handle potentially large HTML\n",
    "                    try:\n",
    "                        import base64\n",
    "                        html_bytes = lime_html_content.encode('utf-8')\n",
    "                        html_base64 = base64.b64encode(html_bytes).decode('utf-8')\n",
    "                        iframe_src = f\"data:text/html;base64,{html_base64}\"\n",
    "                        lime_html = f\"\"\"\n",
    "                        <div style='border: 1px solid #CCC; padding: 10px; border-radius: 5px; background-color: white;'>\n",
    "                            <p style='color: black;'><strong>LIME Explanation (influential words for '{prediction_final}' prediction by Head {selected_task_id}):</strong></p>\n",
    "                            <iframe src=\"{iframe_src}\" width=\"100%\" height=\"450px\" style=\"border:none;\"></iframe>\n",
    "                        </div>\n",
    "                        \"\"\"\n",
    "                    except Exception as e_iframe:\n",
    "                         # handle error during iframe formatting\n",
    "                         print(f\"[Prediction Logic] error creating iframe data URI: {e_iframe}\")\n",
    "                         lime_html = f\"<p style='color:red;'>LIME: Error formatting HTML for display.<br>{e_iframe}</p>\"\n",
    "            else:\n",
    "                # handle case where LIME returns no explanation\n",
    "                print(\"warning:: LIME explain_instance returned None or empty.\")\n",
    "                lime_html = \"<p style='color:orange;'>LIME: explanation returned empty.</p>\"\n",
    "\n",
    "        except Exception as e_lime:\n",
    "             # handle any unexpected error during the LIME generation process\n",
    "             print(f\"[Prediction Logic] LIME error details: {e_lime}\")\n",
    "             traceback.print_exc() # print full traceback to console for debugging\n",
    "             lime_html = f\"<p style='color:red;'><strong>LIME error (exception caught):</strong><br>{e_lime}</p>\"\n",
    "\n",
    "\n",
    "        # step 7: prepare final output strings for Gradio\n",
    "        # step 7a: generate the combined text explanation\n",
    "        explanation_text = (\n",
    "            f\"The text's writing style seems most similar to the **{predicted_domain_name}** dataset.\\n\\n\"\n",
    "            f\"Based on this style, the model predicts the news is **{prediction_final}** with **{confidence_final_str}** confidence.\\n\\n\"\n",
    "            f\"See the LIME explanation below for words influencing this '{prediction_final}' decision.\"\n",
    "        )\n",
    "\n",
    "        # step 7b: generate the detailed technical output string\n",
    "        details_text = (\n",
    "            f\"Predicted Fake Probability: {probs[0]:.1%}\\n\"\n",
    "            f\"Predicted Real Probability: {probs[1]:.1%}\\n\"\n",
    "            f\"(Analysis based on {predicted_domain_name} style)\"\n",
    "        )\n",
    "\n",
    "        # step 7c: assign all results to the output list\n",
    "        outputs = [\n",
    "            prediction_final,\n",
    "            confidence_final_str,\n",
    "            explanation_text,\n",
    "            details_text,\n",
    "            lime_html # the generated LIME html (or error message)\n",
    "        ]\n",
    "        return outputs # return the list of outputs\n",
    "\n",
    "    except Exception as e_outer:\n",
    "        # handle any top-level unexpected errors in the main function\n",
    "        error_msg_outer = f\"An outer error occurred: {e_outer}\"\n",
    "        print(f\"\\n**** [Prediction Logic] TOP LEVEL UNCAUGHT ERROR: {e_outer} ****\")\n",
    "        traceback.print_exc() # print traceback for debugging\n",
    "        outputs[2] = error_msg_outer\n",
    "        # provide context in LIME error message if possible\n",
    "        lime_err_detail = f\"task_id={selected_task_id}\" if selected_task_id else \"selected_task_id might be undefined\"\n",
    "        outputs[4] = f\"<p style='color:red;'>LIME Skipped due to outer error ({lime_err_detail}):<br>{e_outer}</p>\"\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "# this function acts as an intermediary between gradio and the main prediction logic.\n",
    "# it takes the raw text input from gradio and calls the logic function, passing\n",
    "# along the necessary model components captured from the notebook's global scope.\n",
    "# arguments:\n",
    "#   raw_text_input - the text string entered by the user in the gradio interface\n",
    "# returns:\n",
    "#   list of outputs expected by the gradio interface\n",
    "def gradio_interface_fn(raw_text_input):\n",
    "    # step 1: check if all required global objects are loaded and available\n",
    "    if 'mtl_model' in globals() and 'tokenizer' in globals() and \\\n",
    "       'clean_text' in globals() and 'MAX_LEN' in globals() and \\\n",
    "       'device' in globals() and 'domain_classifier' in globals() and \\\n",
    "       'domain_vectorizer' in globals() and \\\n",
    "        mtl_model is not None and tokenizer is not None and \\\n",
    "        callable(clean_text) and device is not None and \\\n",
    "        domain_classifier is not None and domain_vectorizer is not None:\n",
    "\n",
    "        # step 2: if objects are available, call the main prediction logic function\n",
    "        return predict_domain_then_classify_logic(\n",
    "            raw_text=raw_text_input,\n",
    "            mtl_model_obj=mtl_model,       # pass the globally loaded model\n",
    "            bert_tokenizer=tokenizer,    # pass the globally loaded tokenizer\n",
    "            clean_fn=clean_text,         # pass the cleaning function\n",
    "            max_len_val=MAX_LEN,         # pass max sequence length\n",
    "            dev=device,                  # pass the device\n",
    "            domain_clf=domain_classifier,# pass the domain classifier\n",
    "            domain_vec=domain_vectorizer,# pass the domain vectorizer\n",
    "            num_samples_lime=500          # set LIME samples (can be adjusted)\n",
    "        )\n",
    "    else:\n",
    "        # step 3: if objects are missing, return an error state to gradio\n",
    "        error_msg = \"error: Backend components (model, tokenizer, etc.) not loaded properly.\"\n",
    "        print(f\"[gradio_interface_fn] {error_msg}\")\n",
    "        return [\"Error\", \"N/A\", error_msg, \"N/A\", \"<p style='color:red;'>LIME error: backend components missing.</p>\"]\n",
    "\n",
    "\n",
    "# create & launch gradio interface\n",
    "all_objects_ok = True\n",
    "# step 1: verify all necessary global variables/objects exist\n",
    "if 'mtl_model' not in globals() or mtl_model is None: all_objects_ok = False; print(\"error: mtl_model not found or is None.\")\n",
    "if 'tokenizer' not in globals() or tokenizer is None: all_objects_ok = False; print(\"error: tokenizer not found or is None.\")\n",
    "if 'device' not in globals() or device is None: all_objects_ok = False; print(\"error: device not found or is None.\")\n",
    "if 'clean_text' not in globals() or not callable(clean_text): all_objects_ok = False; print(\"error: clean_text function not found or not callable.\")\n",
    "if 'MAX_LEN' not in globals() or not isinstance(MAX_LEN, int): all_objects_ok = False; print(\"error: MAX_LEN not found or not an integer.\")\n",
    "if 'domain_classifier' not in globals() or domain_classifier is None: all_objects_ok = False; print(\"error: domain_classifier not found or is None.\")\n",
    "if 'domain_vectorizer' not in globals() or domain_vectorizer is None: all_objects_ok = False; print(\"error: domain_vectorizer not found or is None.\")\n",
    "\n",
    "# step 2: proceed only if all checks passed\n",
    "if all_objects_ok:\n",
    "    # step 3: create the gradio interface instance\n",
    "        iface = gr.Interface(\n",
    "        fn=gradio_interface_fn, \n",
    "        inputs=gr.Textbox(lines=10, label=\"Enter Text to Classify:\"), \n",
    "        # define output components with updated labels\n",
    "        outputs=[\n",
    "            gr.Textbox(label=\"Prediction\"),\n",
    "            gr.Textbox(label=\"Confidence\"), \n",
    "            gr.Textbox(label=\"Explanation\"),\n",
    "            gr.Textbox(label=\"Prediction Probabilities\"), \n",
    "            gr.HTML(label=\"Word Importance (LIME)\") \n",
    "        ],\n",
    "        title=\"Fake News Detector (MTL-BERT + LIME)\",\n",
    "        description=\"Enter news text below. The model analyzes the writing style, predicts if it's Fake or Real using a specialized analyzer, and shows which words influenced the decision.\", # More user-friendly description\n",
    "        flagging_mode=None\n",
    "    )\n",
    "    \n",
    "    # step 4: launch the interface (add debug=True for more console output from gradio)\n",
    "        iface.launch(share=False, debug=True)\n",
    "else:\n",
    "    # step 5: inform user if launch is skipped due to missing components\n",
    "    print(\"\\nSkipping gradio launch due to missing required global objects.\")\n",
    "    print(\"please ensure model, tokenizer, domain classifier/vectorizer etc. are loaded and available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eced2014-1702-4dc3-bc7d-ff05f9769233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
