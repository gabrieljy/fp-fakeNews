{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeca8652-cd0a-4d6f-a7d2-dc03b7016f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISOT Fake News Detection Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import re\n",
    "import time\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6055230-00a3-4f67-98ea-d6e34cf55e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform minimal text preprocessing for transformer models\n",
    "# arguments:\n",
    "# text - input text to preprocess (string)\n",
    "# max_length - maximum number of words to keep (default: 512)\n",
    "#\n",
    "# returns preprocessed text string\n",
    "def preprocess_text(text, max_length=512):\n",
    "    # step 1: handle edge cases\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # step 2: clean encoding issues\n",
    "    # remove non-ascii characters that could cause problems\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # step 3: normalize text format\n",
    "    # replace multiple whitespace characters with single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # step 4: truncate oversized inputs\n",
    "    # transformer models have context length limits\n",
    "    words = text.split()\n",
    "    if len(words) > max_length:\n",
    "        text = ' '.join(words[:max_length])\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c284845a-72ba-4ed7-a224-4dca519002d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load and preprocess WELFake dataset\n",
    "# arguments:\n",
    "# welfake_path - path to WELFake_Dataset.csv\n",
    "# return - tuple of (train_df, valid_df, test_df) with preprocessed data\n",
    "def load_welfake_dataset(welfake_path=\"data/welfake_dataset/WELFake_Dataset.csv\"):\n",
    "    # step 1: load the raw dataset\n",
    "    print(f\"Loading WELFake dataset from {welfake_path}...\")\n",
    "    welfake_df = pd.read_csv(welfake_path)\n",
    "\n",
    "    # step 2: rename/reshape columns\n",
    "    welfake_df['statement'] = welfake_df['title'] + \" \" + welfake_df['text'].fillna(\"\")\n",
    "    welfake_df['label'] = welfake_df['label']  # Already binary (1=real, 0=fake)\n",
    "    \n",
    "    # step 3: keep only needed columns\n",
    "    welfake_df = welfake_df[['statement', 'label']]\n",
    "    \n",
    "    # step 4: apply preprocessing\n",
    "    print(\"Applying text preprocessing...\")\n",
    "    welfake_df['statement'] = welfake_df['statement'].apply(preprocess_text)\n",
    "\n",
    "    # step 5: do train/valid/test split\n",
    "    train_df, temp_df = train_test_split(welfake_df, test_size=0.2, random_state=42)\n",
    "    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # step 6: print dataset statistics\n",
    "    print(f\"WELFake dataset statistics:\")\n",
    "    print(f\"  Total: {len(welfake_df)} samples\")\n",
    "    print(f\"  Train: {len(train_df)} samples\")\n",
    "    print(f\"  Valid: {len(valid_df)} samples\")\n",
    "    print(f\"  Test: {len(test_df)} samples\")\n",
    "    \n",
    "    # step 7: check class balance\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(f\"  Overall: {welfake_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Train: {train_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Valid: {valid_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Test: {test_df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dd76a1-c89d-4598-b01d-507f2edf9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class for ISOT fake news detection\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        # step 1: store dataframe and extract necessary columns\n",
    "        self.df = df\n",
    "        self.texts = df['statement'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # step 1: get text and label for the index\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # step 2: tokenize the text\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # step 3: return the encodings and label\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e506ecb5-b998-4ec5-85be-c76a7704d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics for model evaluation\n",
    "# arguments:\n",
    "# eval_pred - tuple of (predictions, labels)\n",
    "# return - dictionary with evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'precision': precision_score(labels, predictions, zero_division=0),\n",
    "        'recall': recall_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions),\n",
    "        'roc_auc': roc_auc_score(labels, predictions) if len(np.unique(labels)) > 1 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4561060b-61a1-40c3-9406-e1e196042990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load and preprocess ISOT dataset\n",
    "# arguments:\n",
    "# isot_paths - tuple of paths to (fake_news_path, true_news_path)\n",
    "# return - tuple of (train_df, valid_df, test_df) with preprocessed data\n",
    "def load_isot_dataset(isot_paths=(\"data/isot_dataset/Fake.csv\", \"data/isot_dataset/True.csv\")):\n",
    "    # Handle case where isot_paths is a directory path\n",
    "    if isinstance(isot_paths, str):\n",
    "        fake_path = f\"{isot_paths}/Fake.csv\"\n",
    "        true_path = f\"{isot_paths}/True.csv\"\n",
    "    else:\n",
    "        # Handle case where isot_paths is a tuple of paths\n",
    "        fake_path, true_path = isot_paths\n",
    "    \n",
    "    print(f\"Loading ISOT dataset from {fake_path} and {true_path}...\")\n",
    "    \n",
    "    # Load fake news\n",
    "    fake_df = pd.read_csv(fake_path)\n",
    "    fake_df['label'] = 0  # 0 for fake\n",
    "    \n",
    "    # Load true news\n",
    "    true_df = pd.read_csv(true_path)\n",
    "    true_df['label'] = 1  # 1 for real\n",
    "    \n",
    "    # step 2: combine datasets\n",
    "    isot_df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "    \n",
    "    # step 3: create statement column (title + text)\n",
    "    isot_df['statement'] = isot_df['title'] + \" \" + isot_df['text'].fillna(\"\")\n",
    "    \n",
    "    # step 4: keep only needed columns\n",
    "    isot_df = isot_df[['statement', 'label']]\n",
    "    \n",
    "    # step 5: apply preprocessing\n",
    "    print(\"Applying text preprocessing...\")\n",
    "    isot_df['statement'] = isot_df['statement'].apply(preprocess_text)\n",
    "\n",
    "    # step 6: do train/valid/test split\n",
    "    train_df, temp_df = train_test_split(isot_df, test_size=0.2, random_state=42)\n",
    "    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # step 7: print dataset statistics\n",
    "    print(f\"ISOT dataset statistics:\")\n",
    "    print(f\"  Total: {len(isot_df)} samples\")\n",
    "    print(f\"  Train: {len(train_df)} samples\")\n",
    "    print(f\"  Valid: {len(valid_df)} samples\")\n",
    "    print(f\"  Test: {len(test_df)} samples\")\n",
    "    \n",
    "    # step 8: check class balance\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(f\"  Overall: {isot_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Train: {train_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Valid: {valid_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Test: {test_df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1142972e-822a-4712-930f-1ebb706fd0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curriculum_learning_pipeline(\n",
    "    isot_path=\"data/isot_dataset/\",\n",
    "    welfake_path=\"data/welfake_dataset/WELFake_Dataset.csv\",\n",
    "    model_name=\"distilbert-base-uncased\",\n",
    "    replay_fraction=0.2,\n",
    "    phase1_lr=5e-5,\n",
    "    phase2_lr=5e-6,\n",
    "    batch_size=16,\n",
    "    phase1_epochs=3,\n",
    "    phase2_epochs=2,\n",
    "    seed=42,\n",
    "    output_dir=\"./models/v6_test\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Curriculum learning pipeline with mixed batch replay strategy to prevent catastrophic forgetting.\n",
    "    \n",
    "    Args:\n",
    "        isot_path: Path to ISOT dataset\n",
    "        welfake_path: Path to WELFake dataset\n",
    "        model_name: Pretrained model to use\n",
    "        replay_fraction: Fraction of ISOT samples to keep in replay buffer\n",
    "        phase1_lr: Learning rate for phase 1 (ISOT)\n",
    "        phase2_lr: Learning rate for phase 2 (mixed dataset)\n",
    "        batch_size: Batch size for training\n",
    "        phase1_epochs: Number of epochs for phase 1\n",
    "        phase2_epochs: Number of epochs for phase 2\n",
    "        seed: Random seed\n",
    "        output_dir: Directory to save models\n",
    "        eval_steps: Steps between evaluations\n",
    "        save_steps: Steps between checkpoints\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (trainer, model, tokenizer, results_dict)\n",
    "    \"\"\"\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Load datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    isot_train_df, isot_valid_df, isot_test_df = load_isot_dataset(isot_path)\n",
    "    welfake_train_df, welfake_valid_df, welfake_test_df = load_welfake_dataset(welfake_path)\n",
    "    \n",
    "    # Step 2: Load tokenizer and model\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    \n",
    "    # Step 3: Phase 1 - Train on ISOT dataset\n",
    "    print(\"\\n===== PHASE 1: Training on ISOT dataset =====\")\n",
    "    phase1_output_dir = f\"{output_dir}/phase1\"\n",
    "    \n",
    "    # Create PyTorch datasets\n",
    "    isot_train_dataset = FakeNewsDataset(isot_train_df, tokenizer)\n",
    "    isot_valid_dataset = FakeNewsDataset(isot_valid_df, tokenizer)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args_phase1 = TrainingArguments(\n",
    "        output_dir=phase1_output_dir,\n",
    "        num_train_epochs=phase1_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=phase1_lr,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_steps=save_steps,\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # Define compute metrics function\n",
    "    def compute_metrics(pred):\n",
    "        logits, labels = pred.predictions, pred.label_ids\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        f1 = f1_score(labels, preds)\n",
    "        precision = precision_score(labels, preds)\n",
    "        recall = recall_score(labels, preds)\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer_phase1 = Trainer(\n",
    "        model=model,\n",
    "        args=training_args_phase1,\n",
    "        train_dataset=isot_train_dataset,\n",
    "        eval_dataset=isot_valid_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training on ISOT dataset...\")\n",
    "    trainer_phase1.train()\n",
    "    \n",
    "    # Evaluate on ISOT test dataset\n",
    "    print(\"Evaluating on ISOT test dataset...\")\n",
    "    isot_test_dataset = FakeNewsDataset(isot_test_df, tokenizer)\n",
    "    isot_test_results = trainer_phase1.evaluate(isot_test_dataset)\n",
    "    print(f\"ISOT Test Results: {isot_test_results}\")\n",
    "    \n",
    "    # Step 4: Create ISOT replay buffer for mixed batch training\n",
    "    print(f\"\\nCreating ISOT replay buffer with {replay_fraction*100}% of training samples...\")\n",
    "    isot_replay_df = isot_train_df.sample(frac=replay_fraction, random_state=seed)\n",
    "    print(f\"ISOT replay buffer size: {len(isot_replay_df)} samples\")\n",
    "    \n",
    "    # Step 5: Combine ISOT replay buffer with WELFake training data\n",
    "    print(\"Creating mixed dataset for phase 2...\")\n",
    "    mixed_train_df = pd.concat([welfake_train_df, isot_replay_df])\n",
    "    mixed_train_df = mixed_train_df.sample(frac=1.0, random_state=seed)  # Shuffle\n",
    "    print(f\"Mixed dataset size: {len(mixed_train_df)} samples\")\n",
    "    print(f\"  - WELFake samples: {len(welfake_train_df)}\")\n",
    "    print(f\"  - ISOT replay samples: {len(isot_replay_df)}\")\n",
    "    \n",
    "    # Step 6: Phase 2 - Train on mixed dataset\n",
    "    print(\"\\n===== PHASE 2: Training on mixed dataset =====\")\n",
    "    phase2_output_dir = f\"{output_dir}/phase2\"\n",
    "    \n",
    "    # Create PyTorch datasets for phase 2\n",
    "    mixed_train_dataset = FakeNewsDataset(mixed_train_df, tokenizer)\n",
    "    welfake_valid_dataset = FakeNewsDataset(welfake_valid_df, tokenizer)\n",
    "    \n",
    "    # Training arguments for phase 2\n",
    "    training_args_phase2 = TrainingArguments(\n",
    "        output_dir=phase2_output_dir,\n",
    "        num_train_epochs=phase2_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=phase2_lr,  # Lower learning rate for fine-tuning\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_steps=save_steps,\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Create trainer for phase 2\n",
    "    trainer_phase2 = Trainer(\n",
    "        model=model,  # Continue with the same model from phase 1\n",
    "        args=training_args_phase2,\n",
    "        train_dataset=mixed_train_dataset,\n",
    "        eval_dataset=welfake_valid_dataset,  # Primary validation on WELFake\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    # Train the model on mixed dataset\n",
    "    print(\"Training on mixed dataset...\")\n",
    "    trainer_phase2.train()\n",
    "    \n",
    "    # Step 7: Evaluate the final model on both datasets\n",
    "    print(\"\\n===== FINAL EVALUATION =====\")\n",
    "    \n",
    "    # Evaluate on ISOT test dataset\n",
    "    print(\"Evaluating on ISOT test dataset...\")\n",
    "    isot_final_results = trainer_phase2.evaluate(isot_test_dataset)\n",
    "    print(f\"ISOT Final Results: {isot_final_results}\")\n",
    "    \n",
    "    # Evaluate on WELFake test dataset\n",
    "    print(\"Evaluating on WELFake test dataset...\")\n",
    "    welfake_test_dataset = FakeNewsDataset(welfake_test_df, tokenizer)\n",
    "    welfake_final_results = trainer_phase2.evaluate(welfake_test_dataset)\n",
    "    print(f\"WELFake Final Results: {welfake_final_results}\")\n",
    "    \n",
    "    # Collect all results\n",
    "    results = {\n",
    "        \"isot_phase1\": isot_test_results,\n",
    "        \"isot_final\": isot_final_results,\n",
    "        \"welfake_final\": welfake_final_results\n",
    "    }\n",
    "    \n",
    "    print(\"\\nCurriculum learning with mixed batch replay complete!\")\n",
    "    \n",
    "    return trainer_phase2, model, tokenizer, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9fbaba-9fd5-42ac-ab19-cbe7d5cbe45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loading ISOT dataset from data/isot_dataset//Fake.csv and data/isot_dataset//True.csv...\n",
      "Applying text preprocessing...\n",
      "ISOT dataset statistics:\n",
      "  Total: 44898 samples\n",
      "  Train: 35918 samples\n",
      "  Valid: 4490 samples\n",
      "  Test: 4490 samples\n",
      "\n",
      "Class distribution:\n",
      "  Overall: {0: 23481, 1: 21417}\n",
      "  Train: {0: 18748, 1: 17170}\n",
      "  Valid: {0: 2348, 1: 2142}\n",
      "  Test: {0: 2385, 1: 2105}\n",
      "Loading WELFake dataset from data/welfake_dataset/WELFake_Dataset.csv...\n",
      "Applying text preprocessing...\n",
      "WELFake dataset statistics:\n",
      "  Total: 72134 samples\n",
      "  Train: 57707 samples\n",
      "  Valid: 7213 samples\n",
      "  Test: 7214 samples\n",
      "\n",
      "Class distribution:\n",
      "  Overall: {1: 37106, 0: 35028}\n",
      "  Train: {1: 29768, 0: 27939}\n",
      "  Valid: {1: 3667, 0: 3546}\n",
      "  Test: {1: 3671, 0: 3543}\n",
      "Loading distilbert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Gabriel\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== PHASE 1: Training on ISOT dataset =====\n",
      "Training on ISOT dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='6735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/6735 17:29 < 06:04, 4.76 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>0.998664</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>0.997207</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on ISOT test dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='281' max='281' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [281/281 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISOT Test Results: {'eval_loss': 0.001711554010398686, 'eval_accuracy': 0.999554565701559, 'eval_f1': 0.9995247148288974, 'eval_precision': 1.0, 'eval_recall': 0.9990498812351544, 'eval_runtime': 12.9815, 'eval_samples_per_second': 345.877, 'eval_steps_per_second': 21.646, 'epoch': 2.2271714922048997}\n",
      "\n",
      "Creating ISOT replay buffer with 20.0% of training samples...\n",
      "ISOT replay buffer size: 7184 samples\n",
      "Creating mixed dataset for phase 2...\n",
      "Mixed dataset size: 64891 samples\n",
      "  - WELFake samples: 57707\n",
      "  - ISOT replay samples: 7184\n",
      "\n",
      "===== PHASE 2: Training on mixed dataset =====\n",
      "Training on mixed dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='8112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/8112 1:59:28 < 42:04, 0.84 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.464500</td>\n",
       "      <td>0.232972</td>\n",
       "      <td>0.943020</td>\n",
       "      <td>0.944706</td>\n",
       "      <td>0.932289</td>\n",
       "      <td>0.957458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.351500</td>\n",
       "      <td>0.180583</td>\n",
       "      <td>0.959518</td>\n",
       "      <td>0.959669</td>\n",
       "      <td>0.972292</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.385100</td>\n",
       "      <td>0.176891</td>\n",
       "      <td>0.974629</td>\n",
       "      <td>0.975387</td>\n",
       "      <td>0.962314</td>\n",
       "      <td>0.988819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.184371</td>\n",
       "      <td>0.967697</td>\n",
       "      <td>0.967679</td>\n",
       "      <td>0.984754</td>\n",
       "      <td>0.951186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.158080</td>\n",
       "      <td>0.973520</td>\n",
       "      <td>0.973666</td>\n",
       "      <td>0.984663</td>\n",
       "      <td>0.962912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.322400</td>\n",
       "      <td>0.167348</td>\n",
       "      <td>0.973797</td>\n",
       "      <td>0.973877</td>\n",
       "      <td>0.987388</td>\n",
       "      <td>0.960731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL EVALUATION =====\n",
      "Evaluating on ISOT test dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='732' max='281' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [281/281 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISOT Final Results: {'eval_loss': 1.9121863842010498, 'eval_accuracy': 0.004008908685968819, 'eval_f1': 0.0008936550491510277, 'eval_precision': 0.0008435259384226065, 'eval_recall': 0.0009501187648456057, 'eval_runtime': 12.7239, 'eval_samples_per_second': 352.879, 'eval_steps_per_second': 22.084, 'epoch': 1.4792899408284024}\n",
      "Evaluating on WELFake test dataset...\n",
      "WELFake Final Results: {'eval_loss': 0.1619311422109604, 'eval_accuracy': 0.9804546714721375, 'eval_f1': 0.9810254339927331, 'eval_precision': 0.9694148936170213, 'eval_recall': 0.9929174611822392, 'eval_runtime': 21.4505, 'eval_samples_per_second': 336.31, 'eval_steps_per_second': 21.025, 'epoch': 1.4792899408284024}\n",
      "\n",
      "Curriculum learning with mixed batch replay complete!\n"
     ]
    }
   ],
   "source": [
    "trainer, model, tokenizer, results = curriculum_learning_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "075f6f14-b5a9-4761-bb01-a54b22ab8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text, model, tokenizer, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict if a given news article is real or fake\n",
    "    \"\"\"\n",
    "    # Preprocess text\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        processed_text,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    device = model.device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    fake_prob = probabilities[0, 0].item()\n",
    "    real_prob = probabilities[0, 1].item()\n",
    "    \n",
    "    # Get prediction\n",
    "    predicted_class = 1 if real_prob > threshold else 0\n",
    "    label = \"REAL\" if predicted_class == 1 else \"FAKE\"\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": label,\n",
    "        \"fake_probability\": fake_prob,\n",
    "        \"real_probability\": real_prob,\n",
    "        \"confidence\": max(fake_prob, real_prob)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331adc2-cac2-4b2d-b6c4-6af826f6b533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
