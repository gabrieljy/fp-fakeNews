{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeca8652-cd0a-4d6f-a7d2-dc03b7016f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISOT Fake News Detection Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import re\n",
    "import time\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6055230-00a3-4f67-98ea-d6e34cf55e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform minimal text preprocessing for transformer models\n",
    "# arguments:\n",
    "# text - input text to preprocess (string)\n",
    "# max_length - maximum number of words to keep (default: 512)\n",
    "#\n",
    "# returns preprocessed text string\n",
    "def preprocess_text(text, max_length=512):\n",
    "    # step 1: handle edge cases\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # step 2: clean encoding issues\n",
    "    # remove non-ascii characters that could cause problems\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # step 3: normalize text format\n",
    "    # replace multiple whitespace characters with single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # step 4: truncate oversized inputs\n",
    "    # transformer models have context length limits\n",
    "    words = text.split()\n",
    "    if len(words) > max_length:\n",
    "        text = ' '.join(words[:max_length])\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c284845a-72ba-4ed7-a224-4dca519002d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load and preprocess WELFake dataset\n",
    "# arguments:\n",
    "# welfake_path - path to WELFake_Dataset.csv\n",
    "# return - tuple of (train_df, valid_df, test_df) with preprocessed data\n",
    "def load_welfake_dataset(welfake_path=\"data/welfake_dataset/WELFake_Dataset.csv\"):\n",
    "    # step 1: load the raw dataset\n",
    "    print(f\"Loading WELFake dataset from {welfake_path}...\")\n",
    "    welfake_df = pd.read_csv(welfake_path)\n",
    "\n",
    "    # step 2: rename/reshape columns\n",
    "    welfake_df['statement'] = welfake_df['title'] + \" \" + welfake_df['text'].fillna(\"\")\n",
    "    welfake_df['label'] = welfake_df['label']  # Already binary (1=real, 0=fake)\n",
    "    \n",
    "    # step 3: keep only needed columns\n",
    "    welfake_df = welfake_df[['statement', 'label']]\n",
    "    \n",
    "    # step 4: apply preprocessing\n",
    "    print(\"Applying text preprocessing...\")\n",
    "    welfake_df['statement'] = welfake_df['statement'].apply(preprocess_text)\n",
    "\n",
    "    # step 5: do train/valid/test split\n",
    "    train_df, temp_df = train_test_split(welfake_df, test_size=0.2, random_state=42)\n",
    "    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # step 6: print dataset statistics\n",
    "    print(f\"WELFake dataset statistics:\")\n",
    "    print(f\"  Total: {len(welfake_df)} samples\")\n",
    "    print(f\"  Train: {len(train_df)} samples\")\n",
    "    print(f\"  Valid: {len(valid_df)} samples\")\n",
    "    print(f\"  Test: {len(test_df)} samples\")\n",
    "    \n",
    "    # step 7: check class balance\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(f\"  Overall: {welfake_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Train: {train_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Valid: {valid_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Test: {test_df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dd76a1-c89d-4598-b01d-507f2edf9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class for ISOT fake news detection\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        # step 1: store dataframe and extract necessary columns\n",
    "        self.df = df\n",
    "        self.texts = df['statement'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # step 1: get text and label for the index\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # step 2: tokenize the text\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # step 3: return the encodings and label\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e506ecb5-b998-4ec5-85be-c76a7704d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics for model evaluation\n",
    "# arguments:\n",
    "# eval_pred - tuple of (predictions, labels)\n",
    "# return - dictionary with evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'precision': precision_score(labels, predictions, zero_division=0),\n",
    "        'recall': recall_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions),\n",
    "        'roc_auc': roc_auc_score(labels, predictions) if len(np.unique(labels)) > 1 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4561060b-61a1-40c3-9406-e1e196042990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load and preprocess ISOT dataset\n",
    "# arguments:\n",
    "# isot_paths - tuple of paths to (fake_news_path, true_news_path)\n",
    "# return - tuple of (train_df, valid_df, test_df) with preprocessed data\n",
    "def load_isot_dataset(isot_paths=(\"data/isot_dataset/Fake.csv\", \"data/isot_dataset/True.csv\")):\n",
    "    # step 1: load the raw dataset\n",
    "    fake_path, true_path = isot_paths\n",
    "    print(f\"Loading ISOT dataset from {fake_path} and {true_path}...\")\n",
    "    \n",
    "    # Load fake news\n",
    "    fake_df = pd.read_csv(fake_path)\n",
    "    fake_df['label'] = 0  # 0 for fake\n",
    "    \n",
    "    # Load true news\n",
    "    true_df = pd.read_csv(true_path)\n",
    "    true_df['label'] = 1  # 1 for real\n",
    "    \n",
    "    # step 2: combine datasets\n",
    "    isot_df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "    \n",
    "    # step 3: create statement column (title + text)\n",
    "    isot_df['statement'] = isot_df['title'] + \" \" + isot_df['text'].fillna(\"\")\n",
    "    \n",
    "    # step 4: keep only needed columns\n",
    "    isot_df = isot_df[['statement', 'label']]\n",
    "    \n",
    "    # step 5: apply preprocessing\n",
    "    print(\"Applying text preprocessing...\")\n",
    "    isot_df['statement'] = isot_df['statement'].apply(preprocess_text)\n",
    "\n",
    "    # step 6: do train/valid/test split\n",
    "    train_df, temp_df = train_test_split(isot_df, test_size=0.2, random_state=42)\n",
    "    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # step 7: print dataset statistics\n",
    "    print(f\"ISOT dataset statistics:\")\n",
    "    print(f\"  Total: {len(isot_df)} samples\")\n",
    "    print(f\"  Train: {len(train_df)} samples\")\n",
    "    print(f\"  Valid: {len(valid_df)} samples\")\n",
    "    print(f\"  Test: {len(test_df)} samples\")\n",
    "    \n",
    "    # step 8: check class balance\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(f\"  Overall: {isot_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Train: {train_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Valid: {valid_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Test: {test_df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1142972e-822a-4712-930f-1ebb706fd0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to implement curriculum learning for fake news detection\n",
    "# arguments:\n",
    "# isot_paths - tuple of paths to (fake_news_path, true_news_path) for ISOT dataset\n",
    "# welfake_path - path to WELFake dataset\n",
    "# phase1_epochs - number of epochs for initial ISOT training\n",
    "# phase2_epochs - number of epochs for WELFake fine-tuning\n",
    "#\n",
    "# returns tuple of (trainer, model, tokenizer, evaluation_results)\n",
    "def curriculum_learning_pipeline(\n",
    "    isot_paths=(\"data/isot_dataset/Fake.csv\", \"data/isot_dataset/True.csv\"),\n",
    "    welfake_path=\"data/welfake_dataset/WELFake_Dataset.csv\",\n",
    "    phase1_epochs=3,\n",
    "    phase2_epochs=2,\n",
    "    model_name=\"distilbert-base-uncased\"\n",
    "):\n",
    "    # step 1: load ISOT dataset for phase 1 training\n",
    "    print(\"Loading ISOT dataset for phase 1 (initial training)...\")\n",
    "    isot_train_df, isot_valid_df, isot_test_df = load_isot_dataset(isot_paths)\n",
    "    \n",
    "    # step 2: initialize model and tokenizer\n",
    "    print(f\"Initializing {model_name} model and tokenizer...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=2\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # step 3: create ISOT datasets\n",
    "    print(\"Creating ISOT PyTorch datasets...\")\n",
    "    isot_train_dataset = FakeNewsDataset(isot_train_df, tokenizer, max_length=128)\n",
    "    isot_valid_dataset = FakeNewsDataset(isot_valid_df, tokenizer, max_length=128)\n",
    "    \n",
    "    # step 4: Phase 1 - Train on ISOT dataset\n",
    "    print(\"\\n===== PHASE 1: Training on ISOT dataset =====\")\n",
    "    phase1_training_args = TrainingArguments(\n",
    "        output_dir=\"./isot_pretrained\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=phase1_epochs,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "        fp16=True,  # Use mixed precision for faster training\n",
    "    )\n",
    "    \n",
    "    phase1_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=phase1_training_args,\n",
    "        train_dataset=isot_train_dataset,\n",
    "        eval_dataset=isot_valid_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "    \n",
    "    # Train phase 1\n",
    "    phase1_trainer.train()\n",
    "    \n",
    "    # Evaluate on ISOT after phase 1\n",
    "    print(\"\\nEvaluating Phase 1 model on ISOT test set...\")\n",
    "    isot_test_dataset = FakeNewsDataset(isot_test_df, tokenizer, max_length=128)\n",
    "    phase1_isot_results = phase1_trainer.evaluate(isot_test_dataset)\n",
    "    print(\"ISOT Results after Phase 1:\")\n",
    "    for key, value in phase1_isot_results.items():\n",
    "        if key.startswith(\"eval_\"):\n",
    "            print(f\"  {key[5:]}: {value:.4f}\")\n",
    "    \n",
    "    # step 5: load WELFake dataset for phase 2 training\n",
    "    print(\"\\nLoading WELFake dataset for phase 2 (fine-tuning)...\")\n",
    "    welfake_train_df, welfake_valid_df, welfake_test_df = load_welfake_dataset(welfake_path)\n",
    "    \n",
    "    # step 6: create WELFake datasets\n",
    "    print(\"Creating WELFake PyTorch datasets...\")\n",
    "    welfake_train_dataset = FakeNewsDataset(welfake_train_df, tokenizer, max_length=128)\n",
    "    welfake_valid_dataset = FakeNewsDataset(welfake_valid_df, tokenizer, max_length=128)\n",
    "    welfake_test_dataset = FakeNewsDataset(welfake_test_df, tokenizer, max_length=128)\n",
    "    \n",
    "    # Evaluate on WELFake before phase 2\n",
    "    print(\"\\nEvaluating Phase 1 model on WELFake test set (baseline)...\")\n",
    "    phase1_welfake_results = phase1_trainer.evaluate(welfake_test_dataset)\n",
    "    print(\"WELFake Results before Phase 2:\")\n",
    "    for key, value in phase1_welfake_results.items():\n",
    "        if key.startswith(\"eval_\"):\n",
    "            print(f\"  {key[5:]}: {value:.4f}\")\n",
    "    \n",
    "    # step 7: Phase 2 - Fine-tune on WELFake dataset with smaller learning rate\n",
    "    print(\"\\n===== PHASE 2: Fine-tuning on WELFake dataset =====\")\n",
    "    phase2_training_args = TrainingArguments(\n",
    "        output_dir=\"./welfake_finetuned\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=5e-6,  # Smaller learning rate to avoid catastrophic forgetting\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=phase2_epochs,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    phase2_trainer = Trainer(\n",
    "        model=model,  # Continue with the same model from phase 1\n",
    "        args=phase2_training_args,\n",
    "        train_dataset=welfake_train_dataset,\n",
    "        eval_dataset=welfake_valid_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "    \n",
    "    # Train phase 2\n",
    "    phase2_trainer.train()\n",
    "    \n",
    "    # step 8: Evaluate on both datasets after phase 2\n",
    "    print(\"\\nEvaluating final model on both datasets...\")\n",
    "    \n",
    "    # Evaluate on ISOT\n",
    "    final_isot_results = phase2_trainer.evaluate(isot_test_dataset)\n",
    "    print(\"\\nISOT Results after Phase 2:\")\n",
    "    for key, value in final_isot_results.items():\n",
    "        if key.startswith(\"eval_\"):\n",
    "            print(f\"  {key[5:]}: {value:.4f}\")\n",
    "    \n",
    "    # Evaluate on WELFake\n",
    "    final_welfake_results = phase2_trainer.evaluate(welfake_test_dataset)\n",
    "    print(\"\\nWELFake Results after Phase 2:\")\n",
    "    for key, value in final_welfake_results.items():\n",
    "        if key.startswith(\"eval_\"):\n",
    "            print(f\"  {key[5:]}: {value:.4f}\")\n",
    "    \n",
    "    # step 9: save the final model\n",
    "    model_path = \"curriculum_fake_news_model\"\n",
    "    phase2_trainer.save_model(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    print(f\"\\nFinal model and tokenizer saved to {model_path}\")\n",
    "    \n",
    "    # Prepare evaluation results\n",
    "    evaluation_results = {\n",
    "        \"phase1_isot\": {k.replace('eval_', ''): v for k, v in phase1_isot_results.items() if k.startswith('eval_')},\n",
    "        \"phase1_welfake\": {k.replace('eval_', ''): v for k, v in phase1_welfake_results.items() if k.startswith('eval_')},\n",
    "        \"phase2_isot\": {k.replace('eval_', ''): v for k, v in final_isot_results.items() if k.startswith('eval_')},\n",
    "        \"phase2_welfake\": {k.replace('eval_', ''): v for k, v in final_welfake_results.items() if k.startswith('eval_')}\n",
    "    }\n",
    "    \n",
    "    return phase2_trainer, model, tokenizer, evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9fbaba-9fd5-42ac-ab19-cbe7d5cbe45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ISOT dataset for phase 1 (initial training)...\n",
      "Loading ISOT dataset from data/isot_dataset/Fake.csv and data/isot_dataset/True.csv...\n",
      "Applying text preprocessing...\n",
      "ISOT dataset statistics:\n",
      "  Total: 44898 samples\n",
      "  Train: 35918 samples\n",
      "  Valid: 4490 samples\n",
      "  Test: 4490 samples\n",
      "\n",
      "Class distribution:\n",
      "  Overall: {0: 23481, 1: 21417}\n",
      "  Train: {0: 18748, 1: 17170}\n",
      "  Valid: {0: 2348, 1: 2142}\n",
      "  Test: {0: 2385, 1: 2105}\n",
      "Initializing distilbert-base-uncased model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ISOT PyTorch datasets...\n",
      "\n",
      "===== PHASE 1: Training on ISOT dataset =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6735' max='6735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6735/6735 09:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Phase 1 model on ISOT test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='732' max='281' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [281/281 00:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISOT Results after Phase 1:\n",
      "  loss: 0.0016\n",
      "  accuracy: 0.9998\n",
      "  precision: 1.0000\n",
      "  recall: 0.9995\n",
      "  f1: 0.9998\n",
      "  roc_auc: 0.9998\n",
      "  runtime: 12.1712\n",
      "  samples_per_second: 368.9030\n",
      "  steps_per_second: 23.0870\n",
      "\n",
      "Loading WELFake dataset for phase 2 (fine-tuning)...\n",
      "Loading WELFake dataset from data/welfake_dataset/WELFake_Dataset.csv...\n",
      "Applying text preprocessing...\n",
      "WELFake dataset statistics:\n",
      "  Total: 72134 samples\n",
      "  Train: 57707 samples\n",
      "  Valid: 7213 samples\n",
      "  Test: 7214 samples\n",
      "\n",
      "Class distribution:\n",
      "  Overall: {1: 37106, 0: 35028}\n",
      "  Train: {1: 29768, 0: 27939}\n",
      "  Valid: {1: 3667, 0: 3546}\n",
      "  Test: {1: 3671, 0: 3543}\n",
      "Creating WELFake PyTorch datasets...\n",
      "\n",
      "Evaluating Phase 1 model on WELFake test set (baseline)...\n",
      "WELFake Results before Phase 2:\n",
      "  loss: 7.7914\n",
      "  accuracy: 0.1497\n",
      "  precision: 0.0080\n",
      "  recall: 0.0054\n",
      "  f1: 0.0065\n",
      "  roc_auc: 0.1523\n",
      "  runtime: 21.3556\n",
      "  samples_per_second: 337.8040\n",
      "  steps_per_second: 21.1190\n",
      "\n",
      "===== PHASE 2: Fine-tuning on WELFake dataset =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7214' max='7214' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7214/7214 1:53:30, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.060215</td>\n",
       "      <td>0.983641</td>\n",
       "      <td>0.978689</td>\n",
       "      <td>0.989365</td>\n",
       "      <td>0.983998</td>\n",
       "      <td>0.983543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.068791</td>\n",
       "      <td>0.982393</td>\n",
       "      <td>0.985997</td>\n",
       "      <td>0.979275</td>\n",
       "      <td>0.982624</td>\n",
       "      <td>0.982446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating final model on both datasets...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='732' max='281' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [281/281 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ISOT Results after Phase 2:\n",
      "  loss: 8.5565\n",
      "  accuracy: 0.0013\n",
      "  precision: 0.0004\n",
      "  recall: 0.0005\n",
      "  f1: 0.0004\n",
      "  roc_auc: 0.0013\n",
      "  runtime: 12.9066\n",
      "  samples_per_second: 347.8840\n",
      "  steps_per_second: 21.7720\n",
      "\n",
      "WELFake Results after Phase 2:\n",
      "  loss: 0.0491\n",
      "  accuracy: 0.9864\n",
      "  precision: 0.9824\n",
      "  recall: 0.9910\n",
      "  f1: 0.9867\n",
      "  roc_auc: 0.9863\n",
      "  runtime: 22.2845\n",
      "  samples_per_second: 323.7230\n",
      "  steps_per_second: 20.2380\n",
      "\n",
      "Final model and tokenizer saved to curriculum_fake_news_model\n"
     ]
    }
   ],
   "source": [
    "trainer, model, tokenizer, results = curriculum_learning_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "075f6f14-b5a9-4761-bb01-a54b22ab8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text, model, tokenizer, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict if a given news article is real or fake\n",
    "    \"\"\"\n",
    "    # Preprocess text\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        processed_text,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    device = model.device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    fake_prob = probabilities[0, 0].item()\n",
    "    real_prob = probabilities[0, 1].item()\n",
    "    \n",
    "    # Get prediction\n",
    "    predicted_class = 1 if real_prob > threshold else 0\n",
    "    label = \"REAL\" if predicted_class == 1 else \"FAKE\"\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": label,\n",
    "        \"fake_probability\": fake_prob,\n",
    "        \"real_probability\": real_prob,\n",
    "        \"confidence\": max(fake_prob, real_prob)\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
