{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeca8652-cd0a-4d6f-a7d2-dc03b7016f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISOT Fake News Detection Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import re\n",
    "import time\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6055230-00a3-4f67-98ea-d6e34cf55e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform minimal text preprocessing for transformer models\n",
    "# arguments:\n",
    "# text - input text to preprocess (string)\n",
    "# max_length - maximum number of words to keep (default: 512)\n",
    "#\n",
    "# returns preprocessed text string\n",
    "def preprocess_text(text, max_length=512):\n",
    "    # step 1: handle edge cases\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # step 2: clean encoding issues\n",
    "    # remove non-ascii characters that could cause problems\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # step 3: normalize text format\n",
    "    # replace multiple whitespace characters with single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # step 4: truncate oversized inputs\n",
    "    # transformer models have context length limits\n",
    "    words = text.split()\n",
    "    if len(words) > max_length:\n",
    "        text = ' '.join(words[:max_length])\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c284845a-72ba-4ed7-a224-4dca519002d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load and preprocess WELFake dataset\n",
    "# arguments:\n",
    "# welfake_path - path to WELFake_Dataset.csv\n",
    "# return - tuple of (train_df, valid_df, test_df) with preprocessed data\n",
    "def load_welfake_dataset(welfake_path=\"data/welfake_dataset/WELFake_Dataset.csv\"):\n",
    "    # step 1: load the raw dataset\n",
    "    print(f\"Loading WELFake dataset from {welfake_path}...\")\n",
    "    welfake_df = pd.read_csv(welfake_path)\n",
    "\n",
    "    # step 2: rename/reshape columns\n",
    "    welfake_df['statement'] = welfake_df['title'] + \" \" + welfake_df['text'].fillna(\"\")\n",
    "    welfake_df['label'] = welfake_df['label']  # Already binary (1=real, 0=fake)\n",
    "    \n",
    "    # step 3: keep only needed columns\n",
    "    welfake_df = welfake_df[['statement', 'label']]\n",
    "    \n",
    "    # step 4: apply preprocessing\n",
    "    print(\"Applying text preprocessing...\")\n",
    "    welfake_df['statement'] = welfake_df['statement'].apply(preprocess_text)\n",
    "\n",
    "    # step 5: do train/valid/test split\n",
    "    train_df, temp_df = train_test_split(welfake_df, test_size=0.2, random_state=42)\n",
    "    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # step 6: print dataset statistics\n",
    "    print(f\"WELFake dataset statistics:\")\n",
    "    print(f\"  Total: {len(welfake_df)} samples\")\n",
    "    print(f\"  Train: {len(train_df)} samples\")\n",
    "    print(f\"  Valid: {len(valid_df)} samples\")\n",
    "    print(f\"  Test: {len(test_df)} samples\")\n",
    "    \n",
    "    # step 7: check class balance\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(f\"  Overall: {welfake_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Train: {train_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Valid: {valid_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Test: {test_df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dd76a1-c89d-4598-b01d-507f2edf9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class for ISOT fake news detection\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        # step 1: store dataframe and extract necessary columns\n",
    "        self.df = df\n",
    "        self.texts = df['statement'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # step 1: get text and label for the index\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # step 2: tokenize the text\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # step 3: return the encodings and label\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e506ecb5-b998-4ec5-85be-c76a7704d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics for model evaluation\n",
    "# arguments:\n",
    "# eval_pred - tuple of (predictions, labels)\n",
    "# return - dictionary with evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'precision': precision_score(labels, predictions, zero_division=0),\n",
    "        'recall': recall_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions),\n",
    "        'roc_auc': roc_auc_score(labels, predictions) if len(np.unique(labels)) > 1 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4561060b-61a1-40c3-9406-e1e196042990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load and preprocess ISOT dataset\n",
    "# arguments:\n",
    "# isot_paths - tuple of paths to (fake_news_path, true_news_path)\n",
    "# return - tuple of (train_df, valid_df, test_df) with preprocessed data\n",
    "def load_isot_dataset(isot_paths=(\"data/isot_dataset/Fake.csv\", \"data/isot_dataset/True.csv\")):\n",
    "    # Handle case where isot_paths is a directory path\n",
    "    if isinstance(isot_paths, str):\n",
    "        fake_path = f\"{isot_paths}/Fake.csv\"\n",
    "        true_path = f\"{isot_paths}/True.csv\"\n",
    "    else:\n",
    "        # Handle case where isot_paths is a tuple of paths\n",
    "        fake_path, true_path = isot_paths\n",
    "    \n",
    "    print(f\"Loading ISOT dataset from {fake_path} and {true_path}...\")\n",
    "    \n",
    "    # Load fake news\n",
    "    fake_df = pd.read_csv(fake_path)\n",
    "    fake_df['label'] = 0  # 0 for fake\n",
    "    \n",
    "    # Load true news\n",
    "    true_df = pd.read_csv(true_path)\n",
    "    true_df['label'] = 1  # 1 for real\n",
    "    \n",
    "    # step 2: combine datasets\n",
    "    isot_df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "    \n",
    "    # step 3: create statement column (title + text)\n",
    "    isot_df['statement'] = isot_df['title'] + \" \" + isot_df['text'].fillna(\"\")\n",
    "    \n",
    "    # step 4: keep only needed columns\n",
    "    isot_df = isot_df[['statement', 'label']]\n",
    "    \n",
    "    # step 5: apply preprocessing\n",
    "    print(\"Applying text preprocessing...\")\n",
    "    isot_df['statement'] = isot_df['statement'].apply(preprocess_text)\n",
    "\n",
    "    # step 6: do train/valid/test split\n",
    "    train_df, temp_df = train_test_split(isot_df, test_size=0.2, random_state=42)\n",
    "    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # step 7: print dataset statistics\n",
    "    print(f\"ISOT dataset statistics:\")\n",
    "    print(f\"  Total: {len(isot_df)} samples\")\n",
    "    print(f\"  Train: {len(train_df)} samples\")\n",
    "    print(f\"  Valid: {len(valid_df)} samples\")\n",
    "    print(f\"  Test: {len(test_df)} samples\")\n",
    "    \n",
    "    # step 8: check class balance\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(f\"  Overall: {isot_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Train: {train_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Valid: {valid_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Test: {test_df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1142972e-822a-4712-930f-1ebb706fd0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "import copy\n",
    "\n",
    "def curriculum_learning_pipeline(\n",
    "    isot_path=\"data/isot_dataset/\",\n",
    "    welfake_path=\"data/welfake_dataset/WELFake_Dataset.csv\",\n",
    "    model_name=\"distilbert-base-uncased\",\n",
    "    replay_fraction=0.5,  # Increased from 0.2 to 0.5\n",
    "    phase1_lr=5e-5,\n",
    "    phase2_lr=3e-6,  # Slightly reduced\n",
    "    batch_size=16,\n",
    "    phase1_epochs=3,\n",
    "    phase2_epochs=2,\n",
    "    seed=42,\n",
    "    output_dir=\"./models/curriculum_balanced\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500\n",
    "):\n",
    "    \"\"\"\n",
    "    Curriculum learning pipeline with balanced mixed batch replay strategy to prevent catastrophic forgetting.\n",
    "    \n",
    "    Args:\n",
    "        isot_path: Path to ISOT dataset\n",
    "        welfake_path: Path to WELFake dataset\n",
    "        model_name: Pretrained model to use\n",
    "        replay_fraction: Fraction of ISOT samples to keep in replay buffer\n",
    "        phase1_lr: Learning rate for phase 1 (ISOT)\n",
    "        phase2_lr: Learning rate for phase 2 (mixed dataset)\n",
    "        batch_size: Batch size for training\n",
    "        phase1_epochs: Number of epochs for phase 1\n",
    "        phase2_epochs: Number of epochs for phase 2\n",
    "        seed: Random seed\n",
    "        output_dir: Directory to save models\n",
    "        eval_steps: Steps between evaluations\n",
    "        save_steps: Steps between checkpoints\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (trainer, model, tokenizer, results_dict)\n",
    "    \"\"\"\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Load datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    isot_train_df, isot_valid_df, isot_test_df = load_isot_dataset(isot_path)\n",
    "    welfake_train_df, welfake_valid_df, welfake_test_df = load_welfake_dataset(welfake_path)\n",
    "    \n",
    "    # Step 2: Load tokenizer and model\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    \n",
    "    # Step 3: Phase 1 - Train on ISOT dataset\n",
    "    print(\"\\n===== PHASE 1: Training on ISOT dataset =====\")\n",
    "    phase1_output_dir = f\"{output_dir}/phase1\"\n",
    "    \n",
    "    # Create PyTorch datasets\n",
    "    isot_train_dataset = FakeNewsDataset(isot_train_df, tokenizer)\n",
    "    isot_valid_dataset = FakeNewsDataset(isot_valid_df, tokenizer)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args_phase1 = TrainingArguments(\n",
    "        output_dir=phase1_output_dir,\n",
    "        num_train_epochs=phase1_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=phase1_lr,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_steps=save_steps,\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # Define compute metrics function\n",
    "    def compute_metrics(pred):\n",
    "        logits, labels = pred.predictions, pred.label_ids\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        f1 = f1_score(labels, preds)\n",
    "        precision = precision_score(labels, preds)\n",
    "        recall = recall_score(labels, preds)\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer_phase1 = Trainer(\n",
    "        model=model,\n",
    "        args=training_args_phase1,\n",
    "        train_dataset=isot_train_dataset,\n",
    "        eval_dataset=isot_valid_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training on ISOT dataset...\")\n",
    "    trainer_phase1.train()\n",
    "    \n",
    "    # Save the model from Phase 1\n",
    "    model_p1 = copy.deepcopy(model)\n",
    "    \n",
    "    # Evaluate on ISOT test dataset\n",
    "    print(\"Evaluating on ISOT test dataset...\")\n",
    "    isot_test_dataset = FakeNewsDataset(isot_test_df, tokenizer)\n",
    "    isot_test_results = trainer_phase1.evaluate(isot_test_dataset)\n",
    "    print(f\"ISOT Test Results: {isot_test_results}\")\n",
    "    \n",
    "    # Step 4: Create ISOT replay buffer for mixed batch training\n",
    "    print(f\"\\nCreating ISOT replay buffer with {replay_fraction*100}% of training samples...\")\n",
    "    isot_replay_df = isot_train_df.sample(frac=replay_fraction, random_state=seed)\n",
    "    print(f\"ISOT replay buffer size: {len(isot_replay_df)} samples\")\n",
    "    \n",
    "    # Step 5: Combine ISOT replay buffer with WELFake training data\n",
    "    print(\"Creating mixed dataset for phase 2...\")\n",
    "    mixed_train_df = pd.concat([welfake_train_df, isot_replay_df])\n",
    "    # Add source column to track data origins\n",
    "    if 'source' not in welfake_train_df.columns:\n",
    "        welfake_train_df['source'] = 'welfake'\n",
    "    if 'source' not in isot_replay_df.columns:\n",
    "        isot_replay_df['source'] = 'isot'\n",
    "        \n",
    "    # We'll need these source values for weighted sampling\n",
    "    mixed_train_df = pd.concat([welfake_train_df, isot_replay_df])\n",
    "    mixed_train_df = mixed_train_df.sample(frac=1.0, random_state=seed)  # Shuffle\n",
    "    \n",
    "    print(f\"Mixed dataset size: {len(mixed_train_df)} samples\")\n",
    "    print(f\"  - WELFake samples: {len(welfake_train_df)}\")\n",
    "    print(f\"  - ISOT replay samples: {len(isot_replay_df)}\")\n",
    "    \n",
    "    # Step 6: Create weighted sampler to balance dataset representation\n",
    "    print(\"Creating weighted sampler...\")\n",
    "    # Calculate class weights based on inverse frequency\n",
    "    dataset_counts = mixed_train_df['source'].value_counts()\n",
    "    total_samples = len(mixed_train_df)\n",
    "    num_datasets = len(dataset_counts)\n",
    "    \n",
    "    # Create sample weights array\n",
    "    sample_weights = torch.ones(len(mixed_train_df))\n",
    "    \n",
    "    # Assign weights to balance dataset representation\n",
    "    for i, row in enumerate(mixed_train_df.itertuples()):\n",
    "        source = row.source\n",
    "        weight = total_samples / (num_datasets * dataset_counts[source])\n",
    "        sample_weights[i] = weight\n",
    "    \n",
    "    # Step 7: Create custom DataLoader with weighted sampling\n",
    "    mixed_train_dataset = FakeNewsDataset(mixed_train_df, tokenizer)\n",
    "    welfake_valid_dataset = FakeNewsDataset(welfake_valid_df, tokenizer)\n",
    "    \n",
    "    # Create sampler\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    # Create custom DataLoader with weighted sampling\n",
    "    train_dataloader = DataLoader(\n",
    "        mixed_train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # Step 8: Create custom trainer for phase 2 that uses our weighted DataLoader\n",
    "    class CustomTrainer(Trainer):\n",
    "        def get_train_dataloader(self):\n",
    "            return train_dataloader\n",
    "    \n",
    "    # Step 9: Phase 2 - Train on mixed dataset with weighted sampling\n",
    "    print(\"\\n===== PHASE 2: Training on mixed dataset with weighted sampling =====\")\n",
    "    phase2_output_dir = f\"{output_dir}/phase2\"\n",
    "    \n",
    "    # Training arguments for phase 2\n",
    "    training_args_phase2 = TrainingArguments(\n",
    "        output_dir=phase2_output_dir,\n",
    "        num_train_epochs=phase2_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=phase2_lr,  # Lower learning rate for fine-tuning\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_steps=save_steps,\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Create trainer for phase 2 with weighted sampling\n",
    "    trainer_phase2 = CustomTrainer(\n",
    "        model=model,  # Continue with the same model from phase 1\n",
    "        args=training_args_phase2,\n",
    "        # train_dataset still needed for some trainer functions\n",
    "        train_dataset=mixed_train_dataset,\n",
    "        eval_dataset=welfake_valid_dataset,  # Primary validation on WELFake\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    # Train the model on mixed dataset with weighted sampling\n",
    "    print(\"Training on mixed dataset with weighted sampling...\")\n",
    "    trainer_phase2.train()\n",
    "    \n",
    "    # Step 10: Evaluate the final model on both datasets\n",
    "    print(\"\\n===== FINAL EVALUATION =====\")\n",
    "    \n",
    "    # Evaluate on ISOT test dataset\n",
    "    print(\"Evaluating on ISOT test dataset...\")\n",
    "    isot_final_results = trainer_phase2.evaluate(isot_test_dataset)\n",
    "    print(f\"ISOT Final Results: {isot_final_results}\")\n",
    "    \n",
    "    # Evaluate on WELFake test dataset\n",
    "    print(\"Evaluating on WELFake test dataset...\")\n",
    "    welfake_test_dataset = FakeNewsDataset(welfake_test_df, tokenizer)\n",
    "    welfake_final_results = trainer_phase2.evaluate(welfake_test_dataset)\n",
    "    print(f\"WELFake Final Results: {welfake_final_results}\")\n",
    "    \n",
    "    # Step 11: Compare with Phase 1 model on both datasets\n",
    "    print(\"\\n===== COMPARING PHASE 1 MODEL WITH FINAL MODEL =====\")\n",
    "    \n",
    "    # Restore Phase 1 model\n",
    "    model_p1_eval = model_p1.eval()\n",
    "    \n",
    "    # Create trainer for Phase 1 model evaluation\n",
    "    trainer_p1_eval = Trainer(\n",
    "        model=model_p1_eval,\n",
    "        compute_metrics=compute_metrics,\n",
    "        args=TrainingArguments(output_dir=\"./tmp_eval\", report_to=\"none\")\n",
    "    )\n",
    "    \n",
    "    # Evaluate Phase 1 model on WELFake\n",
    "    print(\"Evaluating Phase 1 model on WELFake test dataset...\")\n",
    "    welfake_p1_results = trainer_p1_eval.evaluate(welfake_test_dataset)\n",
    "    print(f\"WELFake Results from Phase 1 model: {welfake_p1_results}\")\n",
    "    \n",
    "    # Collect all results\n",
    "    results = {\n",
    "        \"isot_phase1\": isot_test_results,\n",
    "        \"isot_final\": isot_final_results,\n",
    "        \"welfake_final\": welfake_final_results,\n",
    "        \"welfake_phase1\": welfake_p1_results\n",
    "    }\n",
    "    \n",
    "    print(\"\\nCurriculum learning with weighted sampling complete!\")\n",
    "    \n",
    "    return trainer_phase2, model, tokenizer, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9fbaba-9fd5-42ac-ab19-cbe7d5cbe45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loading ISOT dataset from data/isot_dataset//Fake.csv and data/isot_dataset//True.csv...\n",
      "Applying text preprocessing...\n",
      "ISOT dataset statistics:\n",
      "  Total: 44898 samples\n",
      "  Train: 35918 samples\n",
      "  Valid: 4490 samples\n",
      "  Test: 4490 samples\n",
      "\n",
      "Class distribution:\n",
      "  Overall: {0: 23481, 1: 21417}\n",
      "  Train: {0: 18748, 1: 17170}\n",
      "  Valid: {0: 2348, 1: 2142}\n",
      "  Test: {0: 2385, 1: 2105}\n",
      "Loading WELFake dataset from data/welfake_dataset/WELFake_Dataset.csv...\n",
      "Applying text preprocessing...\n",
      "WELFake dataset statistics:\n",
      "  Total: 72134 samples\n",
      "  Train: 57707 samples\n",
      "  Valid: 7213 samples\n",
      "  Test: 7214 samples\n",
      "\n",
      "Class distribution:\n",
      "  Overall: {1: 37106, 0: 35028}\n",
      "  Train: {1: 29768, 0: 27939}\n",
      "  Valid: {1: 3667, 0: 3546}\n",
      "  Test: {1: 3671, 0: 3543}\n",
      "Loading distilbert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== PHASE 1: Training on ISOT dataset =====\n",
      "Training on ISOT dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='6735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/6735 14:07 < 17:35, 3.54 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>0.999332</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.999533</td>\n",
       "      <td>0.999066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>0.998664</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>0.997207</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on ISOT test dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='281' max='281' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [281/281 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISOT Test Results: {'eval_loss': 0.0023900421801954508, 'eval_accuracy': 0.999554565701559, 'eval_f1': 0.9995247148288974, 'eval_precision': 1.0, 'eval_recall': 0.9990498812351544, 'eval_runtime': 43.7319, 'eval_samples_per_second': 102.671, 'eval_steps_per_second': 6.426, 'epoch': 1.3363028953229399}\n",
      "\n",
      "Creating ISOT replay buffer with 50.0% of training samples...\n",
      "ISOT replay buffer size: 17959 samples\n",
      "Creating mixed dataset for phase 2...\n",
      "Mixed dataset size: 75666 samples\n",
      "  - WELFake samples: 57707\n",
      "  - ISOT replay samples: 17959\n",
      "Creating weighted sampler...\n",
      "\n",
      "===== PHASE 2: Training on mixed dataset with weighted sampling =====\n",
      "Training on mixed dataset with weighted sampling...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='9460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/9460 1:36:26 < 2:11:42, 0.69 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.683600</td>\n",
       "      <td>0.857497</td>\n",
       "      <td>0.209206</td>\n",
       "      <td>0.065225</td>\n",
       "      <td>0.081725</td>\n",
       "      <td>0.054268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.668400</td>\n",
       "      <td>0.824024</td>\n",
       "      <td>0.253293</td>\n",
       "      <td>0.181210</td>\n",
       "      <td>0.204741</td>\n",
       "      <td>0.162531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.629800</td>\n",
       "      <td>0.721696</td>\n",
       "      <td>0.367531</td>\n",
       "      <td>0.376265</td>\n",
       "      <td>0.377296</td>\n",
       "      <td>0.375239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.609600</td>\n",
       "      <td>0.722120</td>\n",
       "      <td>0.351865</td>\n",
       "      <td>0.350965</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.344696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.587100</td>\n",
       "      <td>0.698964</td>\n",
       "      <td>0.380702</td>\n",
       "      <td>0.403047</td>\n",
       "      <td>0.395178</td>\n",
       "      <td>0.411235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.573100</td>\n",
       "      <td>0.770592</td>\n",
       "      <td>0.345765</td>\n",
       "      <td>0.332248</td>\n",
       "      <td>0.345294</td>\n",
       "      <td>0.320153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.579300</td>\n",
       "      <td>0.672636</td>\n",
       "      <td>0.365035</td>\n",
       "      <td>0.365651</td>\n",
       "      <td>0.371517</td>\n",
       "      <td>0.359967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.581900</td>\n",
       "      <td>0.670912</td>\n",
       "      <td>0.370026</td>\n",
       "      <td>0.375309</td>\n",
       "      <td>0.378431</td>\n",
       "      <td>0.372239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL EVALUATION =====\n",
      "Evaluating on ISOT test dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='732' max='281' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [281/281 01:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISOT Final Results: {'eval_loss': 0.519503653049469, 'eval_accuracy': 0.9367483296213809, 'eval_f1': 0.9367764915405165, 'eval_precision': 0.8814411395056556, 'eval_recall': 0.9995249406175772, 'eval_runtime': 26.9353, 'eval_samples_per_second': 166.696, 'eval_steps_per_second': 10.432, 'epoch': 0.8456659619450317}\n",
      "Evaluating on WELFake test dataset...\n",
      "WELFake Final Results: {'eval_loss': 0.6870463490486145, 'eval_accuracy': 0.371915719434433, 'eval_f1': 0.390748957913137, 'eval_precision': 0.38582049920339884, 'eval_recall': 0.39580495777717245, 'eval_runtime': 48.9, 'eval_samples_per_second': 147.526, 'eval_steps_per_second': 9.223, 'epoch': 0.8456659619450317}\n",
      "\n",
      "===== COMPARING PHASE 1 MODEL WITH FINAL MODEL =====\n",
      "Evaluating Phase 1 model on WELFake test dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='902' max='902' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [902/902 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WELFake Results from Phase 1 model: {'eval_loss': 7.855900764465332, 'eval_model_preparation_time': 0.001, 'eval_accuracy': 0.17604657610202384, 'eval_f1': 0.009993337774816789, 'eval_precision': 0.012858979854264894, 'eval_recall': 0.008172160174339417, 'eval_runtime': 50.0132, 'eval_samples_per_second': 144.242, 'eval_steps_per_second': 18.035}\n",
      "\n",
      "Curriculum learning with weighted sampling complete!\n"
     ]
    }
   ],
   "source": [
    "trainer, model, tokenizer, results = curriculum_learning_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "075f6f14-b5a9-4761-bb01-a54b22ab8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text, model, tokenizer, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict if a given news article is real or fake\n",
    "    \"\"\"\n",
    "    # Preprocess text\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        processed_text,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    device = model.device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    fake_prob = probabilities[0, 0].item()\n",
    "    real_prob = probabilities[0, 1].item()\n",
    "    \n",
    "    # Get prediction\n",
    "    predicted_class = 1 if real_prob > threshold else 0\n",
    "    label = \"REAL\" if predicted_class == 1 else \"FAKE\"\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": label,\n",
    "        \"fake_probability\": fake_prob,\n",
    "        \"real_probability\": real_prob,\n",
    "        \"confidence\": max(fake_prob, real_prob)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331adc2-cac2-4b2d-b6c4-6af826f6b533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
