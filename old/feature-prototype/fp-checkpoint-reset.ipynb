{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d2a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install statements\n",
    "\n",
    "# %pip install transformers \n",
    "# %pip install pandas \n",
    "# %pip install numpy \n",
    "# %pip install scikit-learn \n",
    "# %pip install matplotlib \n",
    "# %pip install shap\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97eb6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id        label                                          statement  \\\n",
      "0   2635.json        false  Says the Annies List political group supports ...   \n",
      "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
      "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
      "3   1123.json        false  Health care reform legislation is likely to ma...   \n",
      "4   9028.json    half-true  The economic turnaround started at the end of ...   \n",
      "\n",
      "                              subject         speaker     speaker_job_title  \\\n",
      "0                            abortion    dwayne-bohac  State representative   \n",
      "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
      "2                      foreign-policy    barack-obama             President   \n",
      "3                         health-care    blog-posting                   NaN   \n",
      "4                        economy,jobs   charlie-crist                   NaN   \n",
      "\n",
      "  state_info party_affiliation  barely_true_counts  false_counts  \\\n",
      "0      Texas        republican                 0.0           1.0   \n",
      "1   Virginia          democrat                 0.0           0.0   \n",
      "2   Illinois          democrat                70.0          71.0   \n",
      "3        NaN              none                 7.0          19.0   \n",
      "4    Florida          democrat                15.0           9.0   \n",
      "\n",
      "   half_true_counts  mostly_true_counts  pants_on_fire_counts  \\\n",
      "0               0.0                 0.0                   0.0   \n",
      "1               1.0                 1.0                   0.0   \n",
      "2             160.0               163.0                   9.0   \n",
      "3               3.0                 5.0                  44.0   \n",
      "4              20.0                19.0                   2.0   \n",
      "\n",
      "               context  \n",
      "0             a mailer  \n",
      "1      a floor speech.  \n",
      "2               Denver  \n",
      "3       a news release  \n",
      "4  an interview on CNN  \n"
     ]
    }
   ],
   "source": [
    "# load dataset to pandas DataFrame\n",
    "\n",
    "# import pandas \\o/ \n",
    "import pandas as pd\n",
    "\n",
    "# load train, test, validation datasets\n",
    "# for the purposes of this demo, we'll be using LIAR dataset :D\n",
    "train_ds = \"liar_dataset/train.tsv\"\n",
    "test_ds = \"liar_dataset/test.tsv\"\n",
    "valid_ds = \"liar_dataset/valid.tsv\"\n",
    "\n",
    "# now, i'll use pandas to read TSV files :D\n",
    "# columns are as according to the README in liar_dataset directory :D\n",
    "\n",
    "columns = [\n",
    "    \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\",\n",
    "    \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\",\n",
    "    \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"\n",
    "]\n",
    "\n",
    "train_df = pd.read_csv(train_ds, sep='\\t', names=columns)\n",
    "test_df = pd.read_csv(test_ds, sep='\\t', names=columns)\n",
    "valid_df = pd.read_csv(valid_ds, sep='\\t', names=columns)\n",
    "\n",
    "# print statement to check the dataset has been loaded properly! T^T\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "817fbf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id  label                                          statement  \\\n",
      "0   2635.json      0  Says the Annies List political group supports ...   \n",
      "1  10540.json      1  When did the decline of coal start? It started...   \n",
      "2    324.json      1  Hillary Clinton agrees with John McCain \"by vo...   \n",
      "3   1123.json      0  Health care reform legislation is likely to ma...   \n",
      "4   9028.json      1  The economic turnaround started at the end of ...   \n",
      "\n",
      "                              subject         speaker     speaker_job_title  \\\n",
      "0                            abortion    dwayne-bohac  State representative   \n",
      "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
      "2                      foreign-policy    barack-obama             President   \n",
      "3                         health-care    blog-posting                   NaN   \n",
      "4                        economy,jobs   charlie-crist                   NaN   \n",
      "\n",
      "  state_info party_affiliation  barely_true_counts  false_counts  \\\n",
      "0      Texas        republican                 0.0           1.0   \n",
      "1   Virginia          democrat                 0.0           0.0   \n",
      "2   Illinois          democrat                70.0          71.0   \n",
      "3        NaN              none                 7.0          19.0   \n",
      "4    Florida          democrat                15.0           9.0   \n",
      "\n",
      "   half_true_counts  mostly_true_counts  pants_on_fire_counts  \\\n",
      "0               0.0                 0.0                   0.0   \n",
      "1               1.0                 1.0                   0.0   \n",
      "2             160.0               163.0                   9.0   \n",
      "3               3.0                 5.0                  44.0   \n",
      "4              20.0                19.0                   2.0   \n",
      "\n",
      "               context  \n",
      "0             a mailer  \n",
      "1      a floor speech.  \n",
      "2               Denver  \n",
      "3       a news release  \n",
      "4  an interview on CNN  \n",
      "           id  label                                          statement  \\\n",
      "0  11972.json      1  Building a wall on the U.S.-Mexico border will...   \n",
      "1  11685.json      0  Wisconsin is on pace to double the number of l...   \n",
      "2  11096.json      0  Says John McCain has done nothing to help the ...   \n",
      "3   5209.json      1  Suzanne Bonamici supports a plan that will cut...   \n",
      "4   9524.json      0  When asked by a reporter whether hes at the ce...   \n",
      "\n",
      "                                             subject  \\\n",
      "0                                        immigration   \n",
      "1                                               jobs   \n",
      "2                    military,veterans,voting-record   \n",
      "3  medicare,message-machine-2012,campaign-adverti...   \n",
      "4  campaign-finance,legal-issues,campaign-adverti...   \n",
      "\n",
      "                            speaker     speaker_job_title state_info  \\\n",
      "0                        rick-perry              Governor      Texas   \n",
      "1                 katrina-shankland  State representative  Wisconsin   \n",
      "2                      donald-trump       President-Elect   New York   \n",
      "3                     rob-cornilles            consultant     Oregon   \n",
      "4  state-democratic-party-wisconsin                   NaN  Wisconsin   \n",
      "\n",
      "  party_affiliation  barely_true_counts  false_counts  half_true_counts  \\\n",
      "0        republican                  30            30                42   \n",
      "1          democrat                   2             1                 0   \n",
      "2        republican                  63           114                51   \n",
      "3        republican                   1             1                 3   \n",
      "4          democrat                   5             7                 2   \n",
      "\n",
      "   mostly_true_counts  pants_on_fire_counts                       context  \n",
      "0                  23                    18               Radio interview  \n",
      "1                   0                     0             a news conference  \n",
      "2                  37                    61  comments on ABC's This Week.  \n",
      "3                   1                     1                  a radio show  \n",
      "4                   2                     7                   a web video  \n",
      "           id  label                                          statement  \\\n",
      "0  12134.json      0  We have less Americans working now than in the...   \n",
      "1    238.json      0  When Obama was sworn into office, he DID NOT u...   \n",
      "2   7891.json      0  Says Having organizations parading as being so...   \n",
      "3   8169.json      1     Says nearly half of Oregons children are poor.   \n",
      "4    929.json      1  On attacks by Republicans that various program...   \n",
      "\n",
      "                            subject          speaker  \\\n",
      "0                      economy,jobs   vicky-hartzler   \n",
      "1  obama-birth-certificate,religion      chain-email   \n",
      "2   campaign-finance,congress,taxes  earl-blumenauer   \n",
      "3                           poverty  jim-francesconi   \n",
      "4                  economy,stimulus     barack-obama   \n",
      "\n",
      "                               speaker_job_title state_info party_affiliation  \\\n",
      "0                            U.S. Representative   Missouri        republican   \n",
      "1                                            NaN        NaN              none   \n",
      "2                            U.S. representative     Oregon          democrat   \n",
      "3  Member of the State Board of Higher Education     Oregon              none   \n",
      "4                                      President   Illinois          democrat   \n",
      "\n",
      "   barely_true_counts  false_counts  half_true_counts  mostly_true_counts  \\\n",
      "0                   1             0                 1                   0   \n",
      "1                  11            43                 8                   5   \n",
      "2                   0             1                 1                   1   \n",
      "3                   0             1                 1                   1   \n",
      "4                  70            71               160                 163   \n",
      "\n",
      "   pants_on_fire_counts                        context  \n",
      "0                     0   an interview with ABC17 News  \n",
      "1                   105                            NaN  \n",
      "2                     0  a U.S. Ways and Means hearing  \n",
      "3                     0             an opinion article  \n",
      "4                     9        interview with CBS News  \n",
      "Unique labels in training data: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# binarising labels! \n",
    "\n",
    "# since the labels have multiple classes, \n",
    "# for the sake of this feature prototype,\n",
    "# i'll just simplify them to binary true/fake labels :)\n",
    "\n",
    "# map labels to binary classes! :D\n",
    "# 'pants-fire', 'false', 'barely-true' -> fake (0)\n",
    "# others -> real (1)\n",
    "\n",
    "def binarise(df):\n",
    "    # validate expected labels exist before applying transformation!\n",
    "    expected_labels = [\"pants-fire\", \"false\", \"barely-true\", \"half-true\", \"mostly-true\", \"true\"]\n",
    "    unexpected_labels = set(df['label']) - set(expected_labels)\n",
    "    if unexpected_labels:\n",
    "        raise ValueError(f\"Unexpected labels found: {unexpected_labels}\")\n",
    "    df['label'] = df['label'].apply(lambda x: 0 if x in ['pants-fire', 'false', 'barely-true'] else 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = binarise(train_df)\n",
    "test_df = binarise(test_df)\n",
    "valid_df = binarise(valid_df)\n",
    "\n",
    "# print statement to check df structure!\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "print(valid_df.head())\n",
    "\n",
    "# checking that all labels in dataset are valid\n",
    "print(\"Unique labels in training data:\", train_df['label'].unique())\n",
    "assert set(train_df['label'].unique()) == {0, 1}, \"Labels must be binary (0 or 1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa89411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token length: 256\n"
     ]
    }
   ],
   "source": [
    "# tokenise statements\n",
    "\n",
    "# i'll tokenise statements using Hugging Face's tokeniser! \n",
    "\n",
    "# import autotokeniser\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load tokeniser\n",
    "tokeniser = AutoTokenizer.from_pretrained(\"google/mobilebert-uncased\")\n",
    "\n",
    "# tokenise data\n",
    "def tokenise(df, tokeniser, max_length=256):\n",
    "    return tokeniser(\n",
    "        df['statement'].tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_encodings = tokenise(train_df, tokeniser)\n",
    "test_encodings = tokenise(test_df, tokeniser)\n",
    "valid_encodings = tokenise(valid_df, tokeniser)\n",
    "\n",
    "print(\"Max token length:\", max([len(ids) for ids in train_encodings['input_ids']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5c436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# fine-tune BERT\n",
    "\n",
    "# now that our dataframes are tokenised, \n",
    "# let's load pre-trained BERT.\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# load our model :D\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google/mobilebert-uncased\", num_labels=2)\n",
    "\n",
    "# prepare our dataset for pytorch! \n",
    "import torch\n",
    "\n",
    "class LIARDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = LIARDataset(train_encodings, train_df['label'].tolist())\n",
    "test_dataset = LIARDataset(test_encodings, test_df['label'].tolist())\n",
    "valid_dataset = LIARDataset(valid_encodings, valid_df['label'].tolist())\n",
    "\n",
    "# finally, we train the model! \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# this codeblock for our dataloader! :D\n",
    "\n",
    "# num_workers to use multiple cpu cores, pin_memory as we are training on GPU\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16, num_workers=4, pin_memory=True)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=16, num_workers=4, pin_memory=True)\n",
    "\n",
    "# this codeblock for optimiser! \n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# this codeblock for scheduler! \n",
    "num_training_steps = len(train_loader) * 5  # 5 epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# this codeblock for device config! \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# this is our training loop. \n",
    "# i will also implement early stopping here\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler  # for mixed-precision training\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "scaler = GradScaler()  # initialise the gradient scaler for mixed-precision training!\n",
    "\n",
    "patience = 3  # stop training after 3 epoch without improvement!\n",
    "best_loss = float('inf')  # track the best loss achieved so far\n",
    "patience_counter = 0\n",
    "\n",
    "accumulation_steps = 2  # Accumulate gradients over 2 steps\n",
    "model.train()\n",
    "\n",
    "# set training params: max epochs\n",
    "max_epochs = 5  \n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_loss = 0  # accumulate epoch loss\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    optimizer.zero_grad()  # reset gradients at start of each epoch\n",
    "\n",
    "    for i, batch in enumerate(loop):\n",
    "        # mve batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # forward pass w/ mixed precision\n",
    "        with autocast():  # this enables mixed precision :D\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / accumulation_steps  # normalise loss for gradient accumulation\n",
    "\n",
    "        # backward pass with gradient scaling for mixed precision\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # gradient clipping. i do this to avoid exploding gradients.\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # after accumulation steps, update model weights & optimiser state\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(loop):\n",
    "            scaler.step(optimizer)  # qpply scaled gradients to optimizer\n",
    "            scaler.update()  # update scaler for next iteration\n",
    "            optimizer.zero_grad()  # after step, reset gradient\n",
    "            lr_scheduler.step()  # this steps learning rate scheduler\n",
    "\n",
    "        # accumulate epoch loss\n",
    "        epoch_loss += loss.item() * accumulation_steps  # Unscale the loss for logging\n",
    "\n",
    "        # progress bar logic\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # log epoch loss \n",
    "    print(f\"Epoch {epoch} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # introduce early stopping logic \n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"as improvements have ceased, triggering early stopping :D\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model!\n",
    "\n",
    "# evaluate the model on test data :D\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=-1).tolist())\n",
    "        true_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "# metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "\n",
    "recall = recall_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# visualise results\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "labels = train_df['label'].unique()\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ef8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# create SHAP explainer using a wrapper function\n",
    "def shap_preprocessing_wrapper(texts):\n",
    "    # tokenise the input texts using the model's tokenizer\n",
    "    tokenized = tokeniser(\n",
    "        texts,  # list of input strings\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # move tokenized inputs to the same device as the model\n",
    "    tokenized = {key: val.to(device) for key, val in tokenized.items()}\n",
    "    # pass the tokenized input through the model and return logits\n",
    "    return model(**tokenized).logits.cpu().detach().numpy()  # Ensure SHAP receives numpy array\n",
    "\n",
    "# generate SHAP explanations\n",
    "try:\n",
    "    # initialize the SHAP explainer with your preprocessing wrapper\n",
    "    explainer = shap.Explainer(shap_preprocessing_wrapper, test_df['statement'].tolist())\n",
    "\n",
    "    # compute SHAP values for the first 10 test samples\n",
    "    shap_values = explainer(test_df['statement'].tolist()[:10])\n",
    "\n",
    "    # visualize SHAP explanations\n",
    "    shap.summary_plot(shap_values)\n",
    "except Exception as e:\n",
    "    print(f\"SHAP explainability failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f739ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()\n",
    "# cuda 12.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913649a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c456b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
